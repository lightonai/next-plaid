//! Benchmark Rust ONNX encoding speed for comparison with PyLate.
//!
//! This benchmark loads the same documents generated by `generate_reference.py --benchmark`
//! and measures encoding speed for fair comparison. It also verifies correctness against
//! PyLate embeddings.
//!
//! Usage:
//!     cargo run --release --bin benchmark_encoding -- --model-dir models/answerai-colbert-small-v1
//!
//! First run the Python benchmark to generate documents:
//!     cd python && python generate_reference.py --benchmark

use anyhow::{Context, Result};
use clap::Parser;
use ndarray::Array2;
use colbert_onnx::Colbert;
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::PathBuf;
use std::time::Instant;

/// Benchmark document data loaded from Python benchmark
#[derive(Deserialize)]
struct BenchmarkDocuments {
    documents: Vec<String>,
    target_tokens: usize,
}

/// Python benchmark results for comparison
#[derive(Deserialize)]
#[allow(dead_code)]
struct PythonBenchmarkResults {
    model_name: String,
    num_docs: usize,
    target_tokens: usize,
    avg_actual_tokens: f64,
    pylate: BenchmarkMetrics,
    onnx_python: BenchmarkMetrics,
    speedup_onnx_over_pylate: f64,
}

/// PyLate verification embedding
#[derive(Deserialize)]
struct VerificationEmbedding {
    doc_index: usize,
    shape: Vec<usize>,
    embeddings: Vec<Vec<f32>>,
}

#[derive(Deserialize, Serialize, Clone)]
struct BenchmarkMetrics {
    total_time_s: f64,
    docs_per_sec: f64,
    ms_per_doc: f64,
}

/// Combined benchmark results
#[derive(Serialize)]
struct CombinedResults {
    model_name: String,
    num_docs: usize,
    target_tokens: usize,
    avg_actual_tokens: f64,
    pylate: BenchmarkMetrics,
    onnx_python: BenchmarkMetrics,
    onnx_rust_sequential: BenchmarkMetrics,
    onnx_rust_batched: BenchmarkMetrics,
    speedup_batched_over_sequential: f64,
    speedup_batched_over_pylate: f64,
    speedup_batched_over_python_onnx: f64,
    correctness_verified: bool,
    avg_cosine_similarity: f64,
    max_abs_difference: f64,
}

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// Path to the model directory
    #[arg(short, long, default_value = "models/answerai-colbert-small-v1")]
    model_dir: PathBuf,

    /// Number of warmup iterations
    #[arg(short, long, default_value_t = 5)]
    warmup: usize,

    /// Number of threads for ONNX Runtime
    #[arg(short, long, default_value_t = 4)]
    threads: usize,

    /// Batch size for batched encoding (0 = all documents in one batch)
    #[arg(short, long, default_value_t = 0)]
    batch_size: usize,

    /// Skip correctness verification
    #[arg(long, default_value_t = false)]
    skip_verification: bool,
}

/// Compute cosine similarity between two vectors
fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm_a < 1e-9 || norm_b < 1e-9 {
        return 0.0;
    }
    dot / (norm_a * norm_b)
}

/// Verify Rust embeddings against PyLate reference embeddings
fn verify_correctness(
    rust_embeddings: &[Array2<f32>],
    verification_path: &PathBuf,
) -> Result<(bool, f64, f64)> {
    if !verification_path.exists() {
        println!("Warning: Verification file not found, skipping correctness check");
        return Ok((true, 1.0, 0.0));
    }

    let content = fs::read_to_string(verification_path)?;
    let pylate_embeddings: Vec<VerificationEmbedding> = serde_json::from_str(&content)?;

    let mut all_similarities: Vec<f64> = Vec::new();
    let mut max_diff: f64 = 0.0;

    println!("\nVerifying correctness against PyLate embeddings...");
    println!("{:<8} {:<12} {:<12} {:<15} {:<15}", "Doc", "Rust Shape", "PyLate Shape", "Avg Cosine Sim", "Max Diff");
    println!("{}", "-".repeat(65));

    for pylate_emb in &pylate_embeddings {
        let doc_idx = pylate_emb.doc_index;
        if doc_idx >= rust_embeddings.len() {
            continue;
        }

        let rust_emb = &rust_embeddings[doc_idx];
        let pylate_rows = &pylate_emb.embeddings;

        // Compare shapes
        let rust_shape = (rust_emb.nrows(), rust_emb.ncols());
        let pylate_shape = (pylate_emb.shape[0], pylate_emb.shape[1]);

        // Compute similarities for each token
        let min_tokens = rust_shape.0.min(pylate_shape.0);
        let mut doc_similarities: Vec<f64> = Vec::new();
        let mut doc_max_diff: f64 = 0.0;

        for i in 0..min_tokens {
            let rust_row: Vec<f32> = rust_emb.row(i).to_vec();
            let pylate_row = &pylate_rows[i];

            let sim = cosine_similarity(&rust_row, pylate_row) as f64;
            doc_similarities.push(sim);

            // Compute max absolute difference
            for (r, p) in rust_row.iter().zip(pylate_row.iter()) {
                let diff = (r - p).abs() as f64;
                if diff > doc_max_diff {
                    doc_max_diff = diff;
                }
            }
        }

        let avg_sim = doc_similarities.iter().sum::<f64>() / doc_similarities.len() as f64;
        all_similarities.push(avg_sim);
        if doc_max_diff > max_diff {
            max_diff = doc_max_diff;
        }

        println!(
            "{:<8} {:>4}x{:<6} {:>4}x{:<6} {:<15.6} {:<15.2e}",
            doc_idx,
            rust_shape.0, rust_shape.1,
            pylate_shape.0, pylate_shape.1,
            avg_sim,
            doc_max_diff
        );
    }

    let overall_avg_sim = all_similarities.iter().sum::<f64>() / all_similarities.len() as f64;
    // Threshold of 0.999 accounts for minor skiplist differences between PyLate and Rust
    let passed = overall_avg_sim > 0.999;

    println!("{}", "-".repeat(65));
    println!("Overall average cosine similarity: {:.6}", overall_avg_sim);
    println!("Maximum absolute difference: {:.2e}", max_diff);

    if passed {
        println!("\nCORRECTNESS CHECK PASSED: Rust ONNX matches PyLate embeddings!");
    } else {
        println!("\nWARNING: Embeddings differ from PyLate (avg sim: {:.6})", overall_avg_sim);
    }

    Ok((passed, overall_avg_sim, max_diff))
}

fn main() -> Result<()> {
    let args = Args::parse();

    println!("{}", "=".repeat(70));
    println!("RUST ONNX ENCODING BENCHMARK (with batching)");
    println!("{}", "=".repeat(70));

    // Load benchmark documents
    let docs_path = args.model_dir.join("benchmark_documents.json");
    if !docs_path.exists() {
        eprintln!("Error: Benchmark documents not found at {:?}", docs_path);
        eprintln!("Please run: cd python && python generate_reference.py --benchmark");
        return Ok(());
    }

    println!("Loading benchmark documents from {:?}...", docs_path);
    let content = fs::read_to_string(&docs_path).context("Failed to read benchmark documents")?;
    let bench_docs: BenchmarkDocuments =
        serde_json::from_str(&content).context("Failed to parse benchmark documents")?;

    let num_docs = bench_docs.documents.len();
    println!("Loaded {} documents (target: {} tokens each)", num_docs, bench_docs.target_tokens);

    // Load Python benchmark results if available
    let results_path = args.model_dir.join("benchmark_results.json");
    let python_results: Option<PythonBenchmarkResults> = if results_path.exists() {
        let content = fs::read_to_string(&results_path)?;
        serde_json::from_str(&content).ok()
    } else {
        None
    };

    if let Some(ref results) = python_results {
        println!("\nPython benchmark results loaded:");
        println!("  PyLate:      {:.1} docs/sec ({:.3} ms/doc)", results.pylate.docs_per_sec, results.pylate.ms_per_doc);
        println!("  ONNX-Python: {:.1} docs/sec ({:.3} ms/doc)", results.onnx_python.docs_per_sec, results.onnx_python.ms_per_doc);
    }

    // Load Rust ONNX model
    println!("\nLoading ONNX model...");
    let mut model = Colbert::from_pretrained_with_threads(&args.model_dir, args.threads)?;
    let config = model.config();
    println!("Config: document_length={}, embedding_dim={}", config.document_length, config.embedding_dim);

    // Convert to &str for encoding
    let docs_refs: Vec<&str> = bench_docs.documents.iter().map(|s| s.as_str()).collect();

    // Warmup
    println!("\nWarming up ({} iterations)...", args.warmup);
    let warmup_docs: Vec<&str> = docs_refs[..args.warmup.min(num_docs)].to_vec();
    let _ = model.encode_documents(&warmup_docs)?;

    // ============ SEQUENTIAL BENCHMARK ============
    println!("\n--- Sequential encoding (one document at a time) ---");
    let start = Instant::now();
    for doc in &docs_refs {
        let _ = model.encode_documents(&[*doc])?;
    }
    let sequential_time = start.elapsed().as_secs_f64();
    let sequential_docs_per_sec = num_docs as f64 / sequential_time;
    println!("  Total time: {:.3}s", sequential_time);
    println!("  Documents/sec: {:.1}", sequential_docs_per_sec);
    println!("  Avg per document: {:.3}ms", 1000.0 * sequential_time / num_docs as f64);

    // ============ BATCHED BENCHMARK ============
    println!("\n--- Batched encoding (optimized) ---");

    let start = Instant::now();
    let batched_embeddings = model.encode_documents(&docs_refs)?;
    let batched_time = start.elapsed().as_secs_f64();
    let batched_docs_per_sec = num_docs as f64 / batched_time;

    println!("  Total time: {:.3}s", batched_time);
    println!("  Documents/sec: {:.1}", batched_docs_per_sec);
    println!("  Avg per document: {:.3}ms", 1000.0 * batched_time / num_docs as f64);

    // ============ CORRECTNESS VERIFICATION ============
    let (correctness_passed, avg_similarity, max_diff) = if !args.skip_verification {
        let verification_path = args.model_dir.join("pylate_verification_embeddings.json");
        verify_correctness(&batched_embeddings, &verification_path)?
    } else {
        println!("\nSkipping correctness verification");
        (true, 1.0, 0.0)
    };

    // ============ SUMMARY ============
    println!("\n{}", "=".repeat(70));
    println!("BENCHMARK SUMMARY");
    println!("{}", "=".repeat(70));
    println!("{:<25} {:<12} {:<12} {:<12}", "Method", "Time (s)", "Docs/sec", "ms/doc");
    println!("{}", "-".repeat(70));

    if let Some(ref results) = python_results {
        println!(
            "{:<25} {:<12.3} {:<12.1} {:<12.3}",
            "PyLate", results.pylate.total_time_s, results.pylate.docs_per_sec, results.pylate.ms_per_doc
        );
        println!(
            "{:<25} {:<12.3} {:<12.1} {:<12.3}",
            "ONNX-Python", results.onnx_python.total_time_s, results.onnx_python.docs_per_sec, results.onnx_python.ms_per_doc
        );
    }
    println!(
        "{:<25} {:<12.3} {:<12.1} {:<12.3}",
        "ONNX-Rust (sequential)", sequential_time, sequential_docs_per_sec, 1000.0 * sequential_time / num_docs as f64
    );
    println!(
        "{:<25} {:<12.3} {:<12.1} {:<12.3}",
        "ONNX-Rust (batched)", batched_time, batched_docs_per_sec, 1000.0 * batched_time / num_docs as f64
    );
    println!("{}", "-".repeat(70));

    // Speedup comparisons
    let speedup_batch_over_seq = sequential_time / batched_time;
    println!("\nSpeedup from batching: {:.2}x", speedup_batch_over_seq);

    if let Some(ref results) = python_results {
        let speedup_over_pylate = results.pylate.total_time_s / batched_time;
        let speedup_over_python_onnx = results.onnx_python.total_time_s / batched_time;

        if speedup_over_pylate > 1.0 {
            println!("ONNX-Rust (batched) is {:.2}x faster than PyLate", speedup_over_pylate);
        } else {
            println!("PyLate is {:.2}x faster than ONNX-Rust (batched)", 1.0 / speedup_over_pylate);
        }

        if speedup_over_python_onnx > 1.0 {
            println!("ONNX-Rust (batched) is {:.2}x faster than ONNX-Python", speedup_over_python_onnx);
        } else {
            println!("ONNX-Python is {:.2}x faster than ONNX-Rust (batched)", 1.0 / speedup_over_python_onnx);
        }

        // Save combined results
        let combined = CombinedResults {
            model_name: results.model_name.clone(),
            num_docs: results.num_docs,
            target_tokens: results.target_tokens,
            avg_actual_tokens: results.avg_actual_tokens,
            pylate: results.pylate.clone(),
            onnx_python: results.onnx_python.clone(),
            onnx_rust_sequential: BenchmarkMetrics {
                total_time_s: sequential_time,
                docs_per_sec: sequential_docs_per_sec,
                ms_per_doc: 1000.0 * sequential_time / num_docs as f64,
            },
            onnx_rust_batched: BenchmarkMetrics {
                total_time_s: batched_time,
                docs_per_sec: batched_docs_per_sec,
                ms_per_doc: 1000.0 * batched_time / num_docs as f64,
            },
            speedup_batched_over_sequential: speedup_batch_over_seq,
            speedup_batched_over_pylate: speedup_over_pylate,
            speedup_batched_over_python_onnx: speedup_over_python_onnx,
            correctness_verified: correctness_passed,
            avg_cosine_similarity: avg_similarity,
            max_abs_difference: max_diff,
        };

        let combined_path = args.model_dir.join("benchmark_combined.json");
        let combined_json = serde_json::to_string_pretty(&combined)?;
        fs::write(&combined_path, combined_json)?;
        println!("\nCombined results saved to {:?}", combined_path);
    }

    if correctness_passed {
        println!("\nCORRECTNESS: VERIFIED (avg cosine similarity: {:.6})", avg_similarity);
    } else {
        println!("\nWARNING: Correctness verification failed!");
    }

    Ok(())
}
