//! Benchmark Rust ONNX encoding speed for comparison with PyLate.
//!
//! This benchmark loads the same documents generated by `generate_reference.py --benchmark`
//! and measures encoding speed for fair comparison.
//!
//! Usage:
//!     cargo run --release --bin benchmark_encoding -- --model-dir models/answerai-colbert-small-v1
//!
//! First run the Python benchmark to generate documents:
//!     cd python && python generate_reference.py --benchmark

use anyhow::{Context, Result};
use clap::Parser;
use onnx_experiment::{ColBertConfig, OnnxColBERT};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::PathBuf;
use std::time::Instant;

/// Benchmark document data loaded from Python benchmark
#[derive(Deserialize)]
struct BenchmarkDocuments {
    documents: Vec<String>,
    target_tokens: usize,
}

/// Python benchmark results for comparison
#[derive(Deserialize)]
#[allow(dead_code)]
struct PythonBenchmarkResults {
    model_name: String,
    num_docs: usize,
    target_tokens: usize,
    avg_actual_tokens: f64,
    pylate: BenchmarkMetrics,
    onnx_python: BenchmarkMetrics,
    speedup_onnx_over_pylate: f64,
}

#[derive(Deserialize, Serialize, Clone)]
struct BenchmarkMetrics {
    total_time_s: f64,
    docs_per_sec: f64,
    ms_per_doc: f64,
}

/// Combined benchmark results
#[derive(Serialize)]
struct CombinedResults {
    model_name: String,
    num_docs: usize,
    target_tokens: usize,
    avg_actual_tokens: f64,
    pylate: BenchmarkMetrics,
    onnx_python: BenchmarkMetrics,
    onnx_rust: BenchmarkMetrics,
    speedup_rust_over_pylate: f64,
    speedup_rust_over_python_onnx: f64,
}

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// Path to the model directory
    #[arg(short, long, default_value = "models/answerai-colbert-small-v1")]
    model_dir: PathBuf,

    /// Number of warmup iterations
    #[arg(short, long, default_value_t = 5)]
    warmup: usize,

    /// Number of threads for ONNX Runtime
    #[arg(short, long, default_value_t = 4)]
    threads: usize,
}

fn main() -> Result<()> {
    let args = Args::parse();

    println!("{}", "=".repeat(70));
    println!("RUST ONNX ENCODING BENCHMARK");
    println!("{}", "=".repeat(70));

    // Load benchmark documents
    let docs_path = args.model_dir.join("benchmark_documents.json");
    if !docs_path.exists() {
        eprintln!("Error: Benchmark documents not found at {:?}", docs_path);
        eprintln!("Please run: cd python && python generate_reference.py --benchmark");
        return Ok(());
    }

    println!("Loading benchmark documents from {:?}...", docs_path);
    let content = fs::read_to_string(&docs_path).context("Failed to read benchmark documents")?;
    let bench_docs: BenchmarkDocuments =
        serde_json::from_str(&content).context("Failed to parse benchmark documents")?;

    let num_docs = bench_docs.documents.len();
    println!("Loaded {} documents (target: {} tokens each)", num_docs, bench_docs.target_tokens);

    // Load Python benchmark results if available
    let results_path = args.model_dir.join("benchmark_results.json");
    let python_results: Option<PythonBenchmarkResults> = if results_path.exists() {
        let content = fs::read_to_string(&results_path)?;
        serde_json::from_str(&content).ok()
    } else {
        None
    };

    if let Some(ref results) = python_results {
        println!("\nPython benchmark results loaded:");
        println!("  PyLate:      {:.1} docs/sec ({:.3} ms/doc)", results.pylate.docs_per_sec, results.pylate.ms_per_doc);
        println!("  ONNX-Python: {:.1} docs/sec ({:.3} ms/doc)", results.onnx_python.docs_per_sec, results.onnx_python.ms_per_doc);
    }

    // Load Rust ONNX model
    println!("\nLoading ONNX model...");
    let config = ColBertConfig::from_model_dir(&args.model_dir).ok();
    let mut model = OnnxColBERT::from_model_dir(&args.model_dir, args.threads)?;

    if let Some(ref cfg) = config {
        println!("Config: document_length={}, embedding_dim={}", cfg.document_length, cfg.embedding_dim);
    }

    // Warmup
    println!("\nWarming up ({} iterations)...", args.warmup);
    let warmup_docs: Vec<&str> = bench_docs.documents[..args.warmup.min(num_docs)]
        .iter()
        .map(|s| s.as_str())
        .collect();
    for doc in &warmup_docs {
        let _ = model.encode(&[*doc], false)?;
    }

    // Benchmark
    println!("\nBenchmarking Rust ONNX ({} documents)...", num_docs);

    // Convert to &str for encoding
    let docs_refs: Vec<&str> = bench_docs.documents.iter().map(|s| s.as_str()).collect();

    // Measure encoding time
    let start = Instant::now();
    for doc in &docs_refs {
        let _ = model.encode(&[*doc], false)?;
    }
    let elapsed = start.elapsed().as_secs_f64();
    let docs_per_sec = num_docs as f64 / elapsed;
    let ms_per_doc = 1000.0 * elapsed / num_docs as f64;

    println!("  Total time: {:.3}s", elapsed);
    println!("  Documents/sec: {:.1}", docs_per_sec);
    println!("  Avg per document: {:.3}ms", ms_per_doc);

    // Summary
    println!("\n{}", "=".repeat(70));
    println!("BENCHMARK SUMMARY");
    println!("{}", "=".repeat(70));
    println!("{:<20} {:<12} {:<12} {:<12}", "Method", "Time (s)", "Docs/sec", "ms/doc");
    println!("{}", "-".repeat(70));

    if let Some(ref results) = python_results {
        println!(
            "{:<20} {:<12.3} {:<12.1} {:<12.3}",
            "PyLate", results.pylate.total_time_s, results.pylate.docs_per_sec, results.pylate.ms_per_doc
        );
        println!(
            "{:<20} {:<12.3} {:<12.1} {:<12.3}",
            "ONNX-Python", results.onnx_python.total_time_s, results.onnx_python.docs_per_sec, results.onnx_python.ms_per_doc
        );
    }
    println!(
        "{:<20} {:<12.3} {:<12.1} {:<12.3}",
        "ONNX-Rust", elapsed, docs_per_sec, ms_per_doc
    );
    println!("{}", "-".repeat(70));

    // Speedup comparisons
    if let Some(ref results) = python_results {
        let speedup_over_pylate = results.pylate.total_time_s / elapsed;
        let speedup_over_python_onnx = results.onnx_python.total_time_s / elapsed;

        println!("\nSpeedup:");
        if speedup_over_pylate > 1.0 {
            println!("  ONNX-Rust is {:.2}x faster than PyLate", speedup_over_pylate);
        } else {
            println!("  PyLate is {:.2}x faster than ONNX-Rust", 1.0 / speedup_over_pylate);
        }

        if speedup_over_python_onnx > 1.0 {
            println!("  ONNX-Rust is {:.2}x faster than ONNX-Python", speedup_over_python_onnx);
        } else {
            println!("  ONNX-Python is {:.2}x faster than ONNX-Rust", 1.0 / speedup_over_python_onnx);
        }

        // Save combined results
        let combined = CombinedResults {
            model_name: results.model_name.clone(),
            num_docs: results.num_docs,
            target_tokens: results.target_tokens,
            avg_actual_tokens: results.avg_actual_tokens,
            pylate: results.pylate.clone(),
            onnx_python: results.onnx_python.clone(),
            onnx_rust: BenchmarkMetrics {
                total_time_s: elapsed,
                docs_per_sec,
                ms_per_doc,
            },
            speedup_rust_over_pylate: speedup_over_pylate,
            speedup_rust_over_python_onnx: speedup_over_python_onnx,
        };

        let combined_path = args.model_dir.join("benchmark_combined.json");
        let combined_json = serde_json::to_string_pretty(&combined)?;
        fs::write(&combined_path, combined_json)?;
        println!("\nCombined results saved to {:?}", combined_path);
    }

    Ok(())
}
