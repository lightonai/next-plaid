{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#nextplaid","title":"NextPlaid","text":"<p>Production-Ready Multi-Vector Search for CPU &amp; GPU</p>"},{"location":"#what-is-nextplaid","title":"What is NextPlaid?","text":"<p>NextPlaid is a production-ready implementation of the PLAID algorithm for efficient multi-vector search. It enables semantic search using late-interaction models like ColBERT, where both documents and queries are represented as multiple vectors (one per token).</p> <p>The PLAID index always runs on CPU using optimized Rust with BLAS acceleration. Model inference for text encoding can run on CPU or GPU depending on the Docker image used.</p> <p>While traditional single-vector search compresses an entire document into one embedding, multi-vector search preserves token-level information for more accurate relevance matching.</p> <p>If you want retrieval running in Python and optimized for GPU, you can rely on FastPlaid.</p>"},{"location":"#quick-example","title":"Quick Example","text":"PythoncURL <pre><code>from next_plaid_client import NextPlaidClient, IndexConfig, SearchParams\n\n# Connect to the API\nclient = NextPlaidClient(\"http://localhost:8080\")\n\n# Create an index\nclient.create_index(\"my_documents\", IndexConfig(nbits=4))\n\n# Add documents with pre-computed embeddings\ndocuments = [\n    {\"embeddings\": [[0.1, 0.2, ...], [0.3, 0.4, ...]]},\n    {\"embeddings\": [[0.5, 0.6, ...], [0.7, 0.8, ...]]}\n]\nmetadata = [\n    {\"title\": \"Document 1\", \"category\": \"science\"},\n    {\"title\": \"Document 2\", \"category\": \"history\"}\n]\nclient.add(\"my_documents\", documents, metadata)\n\n# Search\nresults = client.search(\n    \"my_documents\",\n    queries=[[[0.1, 0.2, ...], [0.3, 0.4, ...]]],\n    params=SearchParams(top_k=10)\n)\n</code></pre> <pre><code># Create an index\ncurl -X POST http://localhost:8080/indices \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"my_documents\", \"config\": {\"nbits\": 4}}'\n\n# Add documents\ncurl -X POST http://localhost:8080/indices/my_documents/documents \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"documents\": [{\"embeddings\": [[0.1, 0.2], [0.3, 0.4]]}]}'\n\n# Search\ncurl -X POST http://localhost:8080/indices/my_documents/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"queries\": [[[0.1, 0.2], [0.3, 0.4]]], \"params\": {\"top_k\": 10}}'\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"Component Description <code>next-plaid</code> Core Rust library for PLAID search <code>next-plaid-api</code> REST API server with Docker support <code>next-plaid-onnx</code> ONNX-based ColBERT encoding <code>next-plaid-client</code> Python SDK for the API <code>pylate-onnx-export</code> CLI tool for model export"},{"location":"#license","title":"License","text":"<p>NextPlaid is licensed under Apache-2.0.</p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@software{next-plaid,\n  title = {NextPlaid: Production-Ready Multi-Vector Search},\n  url = {https://github.com/lightonai/next-plaid},\n  author = {LightOn AI},\n  year = {2025},\n}\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>Complete reference for configuring NextPlaid indices and search.</p>"},{"location":"configuration/#index-configuration","title":"Index Configuration","text":"<p>Configuration used when creating a new index.</p>"},{"location":"configuration/#indexconfig","title":"IndexConfig","text":"<pre><code>from next_plaid_client import IndexConfig\n\nconfig = IndexConfig(\n    nbits=4,\n    batch_size=50000,\n    seed=42,\n    start_from_scratch=999,\n    max_documents=100000,\n)\nclient.create_index(\"my_index\", config)\n</code></pre>"},{"location":"configuration/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>nbits</code> <code>int</code> <code>4</code> Quantization bits for residuals (2 or 4) <code>batch_size</code> <code>int</code> <code>50000</code> Tokens per batch during indexing <code>seed</code> <code>int?</code> <code>42</code> Random seed for reproducibility <code>kmeans_niters</code> <code>int</code> <code>4</code> K-means iterations <code>max_points_per_centroid</code> <code>int</code> <code>256</code> Maximum points per centroid for K-means <code>start_from_scratch</code> <code>int</code> <code>999</code> Rebuild threshold for small indices <code>max_documents</code> <code>int?</code> <code>None</code> Maximum documents allowed"},{"location":"configuration/#nbits","title":"nbits","text":"<p>Controls the precision of residual quantization:</p> Value Levels Storage Quality <code>4</code> 16 0.5 bytes/dim Higher <code>2</code> 4 0.25 bytes/dim Lower <p>Recommendation: Use <code>4</code> unless storage is critical.</p>"},{"location":"configuration/#batch_size","title":"batch_size","text":"<p>Number of tokens processed per batch during indexing. Affects:</p> <ul> <li>Memory usage during indexing</li> <li>Indexing speed</li> </ul> <p>Recommendation: Use default (50000) for most cases.</p>"},{"location":"configuration/#start_from_scratch","title":"start_from_scratch","text":"<p>When updating an index with fewer than this many documents, rebuild from scratch instead of incremental update.</p> <p>Recommendation: Use default (999) for most cases.</p>"},{"location":"configuration/#max_documents","title":"max_documents","text":"<p>Optional limit on total documents. When reached:</p> <ul> <li>New documents are rejected</li> <li>Useful for capacity planning</li> </ul> <p>Set to <code>None</code> for unlimited.</p>"},{"location":"configuration/#search-parameters","title":"Search Parameters","text":"<p>Configuration for search operations.</p>"},{"location":"configuration/#searchparams","title":"SearchParams","text":"<pre><code>from next_plaid_client import SearchParams\n\nparams = SearchParams(\n    top_k=10,\n    n_ivf_probe=8,\n    n_full_scores=4096,\n    centroid_score_threshold=0.4,\n)\nresults = client.search(\"my_index\", queries, params)\n</code></pre>"},{"location":"configuration/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>top_k</code> <code>int</code> <code>10</code> Number of results to return <code>n_ivf_probe</code> <code>int</code> <code>8</code> IVF partitions to search <code>n_full_scores</code> <code>int</code> <code>4096</code> Candidates for exact scoring <code>batch_size</code> <code>int</code> <code>2000</code> Documents per scoring batch <code>centroid_batch_size</code> <code>int</code> <code>100000</code> Batch size for centroid scoring (0 = exhaustive) <code>centroid_score_threshold</code> <code>float?</code> <code>0.4</code> Centroid pruning threshold. Set to <code>None</code> to disable"},{"location":"configuration/#top_k","title":"top_k","text":"<p>Number of documents to return per query.</p>"},{"location":"configuration/#n_ivf_probe","title":"n_ivf_probe","text":"<p>Number of IVF partitions (centroids) to search.</p> <ul> <li>Higher: Better recall, slower</li> <li>Lower: Faster, may miss relevant documents</li> </ul> Value Use Case 4 Fast, lower recall 8 Balanced (default) 16-32 High recall"},{"location":"configuration/#n_full_scores","title":"n_full_scores","text":"<p>Number of candidate documents for exact MaxSim scoring.</p> <ul> <li>Higher: Better ranking accuracy, slower</li> <li>Lower: Faster, may have ranking errors</li> </ul> Value Use Case 1024 Fast, good for small top_k 4096 Balanced (default)"},{"location":"configuration/#centroid_score_threshold","title":"centroid_score_threshold","text":"<p>Centroid pruning threshold that filters out low-scoring centroids during search.</p> <ul> <li>Centroids with max score (across all query tokens) below this threshold are filtered</li> <li>Significantly speeds up search with minimal quality impact</li> <li>Set to <code>None</code> to disable pruning entirely</li> </ul> Value Use Case <code>None</code> Maximum accuracy, slowest 0.35-0.4 High recall for large top_k 0.4 Balanced (default) 0.45-0.5 Faster for small top_k"},{"location":"configuration/#tuning-guidelines","title":"Tuning Guidelines","text":"<p>Maximum quality:</p> <pre><code>SearchParams(\n    top_k=100,\n    n_ivf_probe=32,\n    n_full_scores=16384,\n    centroid_score_threshold=None,  # Disable pruning\n)\n</code></pre> <p>Balanced:</p> <pre><code>SearchParams(\n    top_k=10,\n    n_ivf_probe=8,\n    n_full_scores=4096,\n    centroid_score_threshold=0.4,\n)\n</code></pre> <p>Maximum speed:</p> <pre><code>SearchParams(\n    top_k=10,\n    n_ivf_probe=4,\n    n_full_scores=1024,\n    centroid_score_threshold=0.5,  # Aggressive pruning\n)\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Configure the API server via environment variables.</p>"},{"location":"configuration/#server-configuration","title":"Server Configuration","text":"Variable Default Description <code>RUST_LOG</code> <code>info</code> Log level (trace, debug, info, warn, error) <code>INDEX_DIR</code> <code>/data/indices</code> Index storage directory <code>HOST</code> <code>0.0.0.0</code> Bind address <code>PORT</code> <code>8080</code> Server port"},{"location":"configuration/#model-configuration","title":"Model Configuration","text":"Variable Default Description <code>MODEL</code> <code>None</code> Model path or HuggingFace ID <code>MODELS_DIR</code> <code>/models</code> Model storage directory <code>HF_TOKEN</code> <code>None</code> HuggingFace API token"},{"location":"configuration/#example","title":"Example","text":"<pre><code>docker run -d \\\n  -e RUST_LOG=debug \\\n  -e INDEX_DIR=/data/my_indices \\\n  -e MODEL=lightonai/GTE-ModernColBERT-v1-onnx \\\n  -p 8080:8080 \\\n  ghcr.io/lightonai/next-plaid-api:latest\n</code></pre>"},{"location":"configuration/#cli-arguments","title":"CLI Arguments","text":"<p>The server accepts command-line arguments:</p> <pre><code>next-plaid-api [OPTIONS]\n\nOPTIONS:\n    --index-dir &lt;PATH&gt;    Index storage directory [default: /data/indices]\n    --host &lt;HOST&gt;         Bind address [default: 0.0.0.0]\n    --port &lt;PORT&gt;         Server port [default: 8080]\n    --model &lt;MODEL&gt;       Model path or HuggingFace ID\n</code></pre>"},{"location":"configuration/#example_1","title":"Example","text":"<pre><code>./next-plaid-api \\\n  --index-dir ./my_indices \\\n  --port 9090 \\\n  --model lightonai/GTE-ModernColBERT-v1-onnx\n</code></pre>"},{"location":"configuration/#rate-limiting","title":"Rate Limiting","text":"<p>The API implements token bucket rate limiting:</p> Setting Value Sustained rate 50 requests/second Burst limit 100 requests <p>When rate limited, the API returns <code>429 Too Many Requests</code>.</p>"},{"location":"configuration/#memory-configuration","title":"Memory Configuration","text":""},{"location":"configuration/#estimating-memory-usage","title":"Estimating Memory Usage","text":"<p>Memory usage depends on:</p> <ol> <li>Number of documents</li> <li>Average tokens per document</li> <li>Embedding dimension</li> <li>Quantization bits</li> </ol> <p>Formula:</p> <pre><code>Base: ~100 MB\nPer document: ~(avg_tokens \u00d7 dim \u00d7 nbits / 8) bytes\nCentroids: ~(num_partitions \u00d7 dim \u00d7 4) bytes\n</code></pre> <p>Example for 1M documents:</p> <pre><code>1M docs \u00d7 15 tokens \u00d7 128 dim \u00d7 4 bits / 8 = 960 MB\n+ Centroids: 16K \u00d7 128 \u00d7 4 = 8 MB\n+ Overhead: ~100 MB\nTotal: ~1.1 GB\n</code></pre>"},{"location":"configuration/#memory-mapped-indices","title":"Memory-Mapped Indices","text":"<p>For large indices, NextPlaid supports memory mapping:</p> <ul> <li>Index size can exceed available RAM</li> <li>OS manages page swapping</li> <li>Multiple processes can share the index</li> </ul>"},{"location":"configuration/#docker-configuration","title":"Docker Configuration","text":""},{"location":"configuration/#volume-mounts","title":"Volume Mounts","text":"Path Purpose <code>/data/indices</code> Index storage <code>/models</code> Model cache"},{"location":"configuration/#resource-limits","title":"Resource Limits","text":"<pre><code>services:\n  next-plaid:\n    deploy:\n      resources:\n        limits:\n          memory: 8G\n        reservations:\n          memory: 1G\n</code></pre> <p>See Docker Deployment for complete configuration.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to NextPlaid!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust 1.70+ (rustup)</li> <li>Python 3.10+ with uv</li> <li>Docker 20.10+</li> <li>OpenBLAS (Linux) or Accelerate (macOS)</li> </ul>"},{"location":"contributing/#clone-and-setup","title":"Clone and Setup","text":"<pre><code>git clone https://github.com/lightonai/next-plaid.git\ncd next-plaid\n\n# Install git hooks\nmake install-hooks\n\n# Install OpenBLAS (Linux)\nsudo apt-get install -y libopenblas-dev\n\n# Run all CI checks\nmake ci\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>next-plaid/\n\u251c\u2500\u2500 next-plaid/           # Core library (Rust)\n\u251c\u2500\u2500 next-plaid-api/       # REST API server (Rust)\n\u2502   \u251c\u2500\u2500 python-sdk/       # Python client SDK\n\u2502   \u2514\u2500\u2500 Dockerfile        # Multi-stage Docker build\n\u251c\u2500\u2500 next-plaid-onnx/      # ONNX encoding support (Rust)\n\u2502   \u2514\u2500\u2500 python/           # pylate-onnx-export CLI tool\n\u251c\u2500\u2500 benchmarks/           # Evaluation benchmarks\n\u2514\u2500\u2500 .github/workflows/    # CI/CD pipelines\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#building","title":"Building","text":"<pre><code># Debug build\nmake build\n\n# Release build\nmake release\n\n# Build with specific features\ncargo build --release -p next-plaid --features \"openblas\"\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code># Run all Rust tests\nmake test\n\n# Run with specific features\ncargo test --features \"openblas\"\n\n# Run API integration tests\nmake test-api\n\n# Run Python SDK tests\ncd next-plaid-api/python-sdk &amp;&amp; pytest tests/\n</code></pre>"},{"location":"contributing/#linting-and-formatting","title":"Linting and Formatting","text":"<pre><code># Format code\nmake fmt\n\n# Run clippy\nmake lint\n\n# Format and lint Python\nmake fmt-python\nmake lint-python\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<pre><code># Build Rust documentation\nmake doc\n\n# Open in browser\ncargo doc --open\n</code></pre>"},{"location":"contributing/#docker-development","title":"Docker Development","text":""},{"location":"contributing/#building-images","title":"Building Images","text":"<pre><code># CPU-only image\nmake docker-build\n\n# With CUDA support\ndocker build -t next-plaid-api:cuda \\\n  -f next-plaid-api/Dockerfile \\\n  --target runtime-cuda .\n</code></pre>"},{"location":"contributing/#running-locally","title":"Running Locally","text":"<pre><code># Start with Docker Compose\nmake docker-up\n\n# View logs\nmake docker-logs\n\n# Stop\nmake docker-down\n</code></pre>"},{"location":"contributing/#benchmarking","title":"Benchmarking","text":"<pre><code># Run SciFact benchmark\nmake benchmark-scifact-update\n\n# Compare with FastPlaid\nmake compare-scifact-cached\n\n# Evaluate retrieval quality\nmake evaluate-scifact-cached\n</code></pre>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ol> <li>Create a feature branch from <code>main</code></li> <li>Write tests for new functionality</li> <li>Run CI checks locally: <code>make ci</code></li> <li>Keep commits focused - one logical change per commit</li> <li>Write clear commit messages</li> </ol>"},{"location":"contributing/#commit-message-format","title":"Commit Message Format","text":"<pre><code>type(scope): description\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>style</code>, <code>refactor</code>, <code>test</code>, <code>chore</code></p> <p>Examples:</p> <pre><code>feat(api): add batch encoding endpoint\nfix(search): correct score normalization for empty queries\ndocs: update installation instructions\nchore: bump dependencies\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#rust","title":"Rust","text":"<ul> <li>Follow standard Rust conventions</li> <li>Run <code>cargo fmt</code> before committing</li> <li>Address all <code>clippy</code> warnings</li> <li>Document public APIs with doc comments</li> </ul>"},{"location":"contributing/#python","title":"Python","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Format with <code>black</code> and <code>isort</code></li> <li>Lint with <code>ruff</code></li> </ul>"},{"location":"contributing/#testing-requirements","title":"Testing Requirements","text":""},{"location":"contributing/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test individual functions and modules</li> <li>Use descriptive test names</li> <li>Cover edge cases</li> </ul>"},{"location":"contributing/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test API endpoints end-to-end</li> <li>Test Python SDK against running server</li> <li>Use fixtures for test data</li> </ul>"},{"location":"contributing/#benchmarks","title":"Benchmarks","text":"<ul> <li>Run benchmarks before and after changes</li> <li>Document performance impact in PR</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>Releases are automated via GitHub Actions when a version tag is pushed.</p>"},{"location":"contributing/#what-gets-published","title":"What Gets Published","text":"Package Registry <code>next-plaid</code> crates.io <code>next-plaid-onnx</code> crates.io <code>next-plaid-client</code> PyPI <code>pylate-onnx-export</code> PyPI <code>next-plaid-api</code> GitHub Container Registry"},{"location":"contributing/#creating-a-release","title":"Creating a Release","text":"<ol> <li> <p>Update version numbers in:</p> </li> <li> <p><code>/Cargo.toml</code> (workspace version)</p> </li> <li><code>/next-plaid-api/python-sdk/pyproject.toml</code></li> <li> <p><code>/next-plaid-onnx/python/pyproject.toml</code></p> </li> <li> <p>Commit the version bump:</p> </li> </ol> <pre><code>git add -A\ngit commit -m \"chore: release v0.2.0\"\n</code></pre> <ol> <li>Create and push the tag:</li> </ol> <pre><code>git tag v0.2.0\ngit push origin main --tags\n</code></pre> <ol> <li>Monitor the release in GitHub Actions</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache-2.0 license.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get NextPlaid running in under 5 minutes with Docker.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed</li> <li>Python 3.8+ (for the SDK)</li> </ul>"},{"location":"getting-started/#step-1-start-the-server","title":"Step 1: Start the Server","text":"<p>Start the server using the pre-built Docker image:</p> <pre><code># Pull and run the latest image\ndocker run -d \\\n  --name next-plaid-api \\\n  -p 8080:8080 \\\n  -v ~/.local/share/next-plaid:/data/indices \\\n  ghcr.io/lightonai/next-plaid-api:latest\n</code></pre> <p>Verify it's running:</p> <pre><code>curl http://localhost:8080/health\n</code></pre> <p>You should see:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"loaded_indices\": 0,\n  \"index_dir\": \"/data/indices\",\n  \"memory_usage_bytes\": 12345678,\n  \"indices\": []\n}\n</code></pre>"},{"location":"getting-started/#step-2-install-the-python-sdk","title":"Step 2: Install the Python SDK","text":"<pre><code>pip install next-plaid-client\n</code></pre>"},{"location":"getting-started/#step-3-create-your-first-index","title":"Step 3: Create Your First Index","text":"<pre><code>from next_plaid_client import NextPlaidClient, IndexConfig\n\n# Connect to the API\nclient = NextPlaidClient(\"http://localhost:8080\")\n\n# Check the server is healthy\nhealth = client.health()\nprint(f\"Server status: {health.status}\")\n\n# Create an index with 4-bit quantization\nclient.create_index(\"my_index\", IndexConfig(nbits=4))\n\n# List indices\nprint(client.list_indices())  # ['my_index']\n</code></pre>"},{"location":"getting-started/#step-4-add-documents","title":"Step 4: Add Documents","text":"<p>Documents are represented as multi-vector embeddings. Each document is a list of token embeddings.</p> <pre><code>import numpy as np\n\n# Generate some example embeddings\n# In practice, you'd use a ColBERT model to encode your documents\ndim = 128\ndocuments = []\nfor i in range(10):\n    num_tokens = np.random.randint(5, 20)  # Variable length\n    embeddings = np.random.randn(num_tokens, dim).tolist()\n    documents.append({\"embeddings\": embeddings})\n\n# Optional: add metadata for filtering\nmetadata = [{\"title\": f\"Document {i}\", \"category\": \"example\"} for i in range(10)]\n\n# Add to index\nclient.add(\"my_index\", documents, metadata)\n\n# Check the index\ninfo = client.get_index(\"my_index\")\nprint(f\"Documents: {info.num_documents}\")\nprint(f\"Total embeddings: {info.num_embeddings}\")\n</code></pre>"},{"location":"getting-started/#step-5-search","title":"Step 5: Search","text":"<pre><code>from next_plaid_client import SearchParams\n\n# Generate a query (in practice, use a ColBERT model)\nquery_tokens = 8\nquery = [np.random.randn(query_tokens, dim).tolist()]\n\n# Search\nresults = client.search(\n    \"my_index\",\n    queries=query,\n    params=SearchParams(top_k=5)\n)\n\n# Print results\nfor result in results.results:\n    print(f\"Query {result.query_id}:\")\n    for doc_id, score in zip(result.document_ids, result.scores):\n        print(f\"  Doc {doc_id}: {score:.4f}\")\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Installation - More installation options</li> <li>Python SDK - Full SDK reference</li> <li>REST API - API documentation</li> <li>Concepts - Learn about multi-vector search</li> <li>Model Export - Use real ColBERT models</li> </ul>"},{"location":"getting-started/#using-text-queries","title":"Using Text Queries","text":"<p>Both CPU and CUDA Docker images support model inference. To encode text directly instead of providing embeddings, start the server with a model:</p> <pre><code>docker run -d \\\n  --name next-plaid-api \\\n  -p 8080:8080 \\\n  -v ~/.local/share/next-plaid:/data/indices \\\n  -v next-plaid-models:/models \\\n  ghcr.io/lightonai/next-plaid-api:latest \\\n  --model lightonai/GTE-ModernColBERT-v1-onnx\n</code></pre> <p>Then use text input directly (the <code>add</code> and <code>search</code> methods auto-detect text vs embeddings):</p> <pre><code># Add documents with text (auto-detected)\nclient.add(\n    \"my_index\",\n    [\"Paris is the capital of France.\", \"Berlin is in Germany.\"],\n    metadata=[{\"country\": \"France\"}, {\"country\": \"Germany\"}]\n)\n\n# Search with text (auto-detected)\nresults = client.search(\n    \"my_index\",\n    queries=[\"What is the capital of France?\"],\n    params=SearchParams(top_k=5)\n)\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Choose the installation method that best fits your use case.</p>"},{"location":"installation/#docker-recommended","title":"Docker (Recommended)","text":"<p>Run NextPlaid in production using Docker containers. Both images support model inference for text encoding. The PLAID index and search always run on CPU, while model inference runs on CPU or GPU depending on the image.</p>"},{"location":"installation/#pre-built-images","title":"Pre-built Images","text":"CPUCUDA <pre><code>docker pull ghcr.io/lightonai/next-plaid-api:latest\n\ndocker run -d \\\n  --name next-plaid-api \\\n  -p 8080:8080 \\\n  -v ~/.local/share/next-plaid:/data/indices \\\n  -v next-plaid-models:/models \\\n  ghcr.io/lightonai/next-plaid-api:latest \\\n  --model lightonai/GTE-ModernColBERT-v1-onnx\n</code></pre> <pre><code>docker pull ghcr.io/lightonai/next-plaid-api:latest-cuda\n\ndocker run -d \\\n  --name next-plaid-api \\\n  --gpus all \\\n  -p 8080:8080 \\\n  -v ~/.local/share/next-plaid:/data/indices \\\n  -v next-plaid-models:/models \\\n  ghcr.io/lightonai/next-plaid-api:latest-cuda \\\n  --model lightonai/GTE-ModernColBERT-v1-onnx\n</code></pre>"},{"location":"installation/#docker-compose","title":"Docker Compose","text":"<p>Clone the repository and use Docker Compose:</p> <pre><code>git clone https://github.com/lightonai/next-plaid.git\ncd next-plaid\n\n# CPU only\ndocker compose up -d\n\n# With CUDA support\ndocker compose -f docker-compose.yml -f docker-compose.cuda.yml up -d\n</code></pre>"},{"location":"installation/#image-tags","title":"Image Tags","text":"<p>The index and search always run on CPU. Model inference runs on CPU or GPU depending on the image.</p> Tag Description Model Inference <code>latest</code> Latest CPU release CPU <code>X.Y.Z</code> Specific version (CPU) CPU <code>latest-cuda</code> Latest CUDA release GPU <code>X.Y.Z-cuda</code> Specific version (CUDA) GPU"},{"location":"installation/#python-sdk","title":"Python SDK","text":"<p>Install the Python client library:</p> <pre><code>pip install next-plaid-client\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 - 3.12</li> <li>httpx (installed automatically)</li> </ul>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from next_plaid_client import NextPlaidClient\n\nclient = NextPlaidClient(\"http://localhost:8080\")\nprint(client.health())\n</code></pre>"},{"location":"installation/#rust-crate","title":"Rust Crate","text":"<p>For Rust developers who want to embed NextPlaid directly:</p>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<pre><code>[dependencies]\nnext-plaid = { git = \"https://github.com/lightonai/next-plaid\" }\n</code></pre>"},{"location":"installation/#with-blas-acceleration-recommended","title":"With BLAS Acceleration (Recommended)","text":"macOS (Accelerate)Linux (OpenBLAS) <pre><code>[dependencies]\nnext-plaid = {\n  git = \"https://github.com/lightonai/next-plaid\",\n  features = [\"accelerate\"]\n}\n</code></pre> <p>First install OpenBLAS:</p> <pre><code># Ubuntu/Debian\nsudo apt-get install -y libopenblas-dev\n\n# Fedora\nsudo dnf install openblas-devel\n</code></pre> <p>Then add to Cargo.toml:</p> <pre><code>[dependencies]\nnext-plaid = {\n  git = \"https://github.com/lightonai/next-plaid\",\n  features = [\"openblas\"]\n}\n</code></pre>"},{"location":"installation/#feature-flags","title":"Feature Flags","text":"Feature Description <code>accelerate</code> macOS BLAS acceleration <code>openblas</code> Linux OpenBLAS acceleration"},{"location":"installation/#model-export-tool","title":"Model Export Tool","text":"<p>Install the CLI tool for exporting ColBERT models to ONNX:</p> <pre><code>pip install pylate-onnx-export\n</code></pre>"},{"location":"installation/#requirements_1","title":"Requirements","text":"<ul> <li>Python 3.10 - 3.12</li> <li>PyTorch</li> <li>Transformers</li> <li>ONNX Runtime</li> </ul>"},{"location":"installation/#verify-installation_1","title":"Verify Installation","text":"<pre><code>pylate-onnx-export --help\n</code></pre> <p>See Model Export for usage details.</p>"},{"location":"installation/#building-from-source","title":"Building from Source","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust 1.70+</li> <li>Python 3.10+ with uv</li> <li>OpenBLAS (Linux) or Accelerate (macOS)</li> </ul>"},{"location":"installation/#build-steps","title":"Build Steps","text":"<pre><code># Clone the repository\ngit clone https://github.com/lightonai/next-plaid.git\ncd next-plaid\n\n# Build the API server (Linux)\ncargo build --release -p next-plaid-api --features \"openblas\"\n\n# Build the API server (macOS)\ncargo build --release -p next-plaid-api --features \"accelerate\"\n\n# Run the server\n./target/release/next-plaid-api --index-dir ./indices\n</code></pre>"},{"location":"installation/#build-docker-images-locally","title":"Build Docker Images Locally","text":"<pre><code># CPU variant\ndocker build -t next-plaid-api:local \\\n  -f next-plaid-api/Dockerfile \\\n  --target runtime-cpu .\n\n# CUDA variant\ndocker build -t next-plaid-api:local-cuda \\\n  -f next-plaid-api/Dockerfile \\\n  --target runtime-cuda .\n</code></pre>"},{"location":"model-export/","title":"Model Export","text":"<p>Export ColBERT models to ONNX format for use with NextPlaid.</p>"},{"location":"model-export/#overview","title":"Overview","text":"<p>NextPlaid can encode text using ONNX models. The <code>pylate-onnx-export</code> tool converts PyLate-compatible ColBERT models from HuggingFace to ONNX format.</p>"},{"location":"model-export/#installation","title":"Installation","text":"<pre><code>pip install pylate-onnx-export\n</code></pre>"},{"location":"model-export/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 - 3.12</li> <li>PyTorch</li> <li>Transformers</li> <li>ONNX Runtime</li> </ul>"},{"location":"model-export/#basic-usage","title":"Basic Usage","text":""},{"location":"model-export/#export-a-model","title":"Export a Model","text":"<pre><code>pylate-onnx-export lightonai/GTE-ModernColBERT-v1\n</code></pre> <p>This creates a directory with:</p> <pre><code>GTE-ModernColBERT-v1/\n\u251c\u2500\u2500 model.onnx                        # FP32 ONNX model\n\u251c\u2500\u2500 model_int8.onnx                   # INT8 quantized (with --quantize)\n\u251c\u2500\u2500 tokenizer.json                    # Tokenizer\n\u2514\u2500\u2500 config_sentence_transformers.json # Model config\n</code></pre>"},{"location":"model-export/#export-with-quantization","title":"Export with Quantization","text":"<p>INT8 quantization reduces model size and improves inference speed:</p> <pre><code>pylate-onnx-export lightonai/GTE-ModernColBERT-v1 --quantize\n</code></pre>"},{"location":"model-export/#export-to-custom-directory","title":"Export to Custom Directory","text":"<pre><code>pylate-onnx-export lightonai/GTE-ModernColBERT-v1 -o ./models\n</code></pre>"},{"location":"model-export/#command-reference","title":"Command Reference","text":"<pre><code>pylate-onnx-export [OPTIONS] MODEL_NAME\n\nArguments:\n  MODEL_NAME    HuggingFace model ID (e.g., \"lightonai/GTE-ModernColBERT-v1\")\n\nOptions:\n  -o, --output DIR        Output directory [default: ./]\n  --quantize              Apply INT8 quantization\n  --push-to-hub REPO      Push to HuggingFace Hub\n  --help                  Show this message\n</code></pre>"},{"location":"model-export/#push-to-huggingface-hub","title":"Push to HuggingFace Hub","text":"<p>Export and upload directly to HuggingFace:</p> <pre><code># Login first\nhuggingface-cli login\n\n# Export and push\npylate-onnx-export lightonai/GTE-ModernColBERT-v1 \\\n  --quantize \\\n  --push-to-hub myorg/my-model-onnx\n</code></pre>"},{"location":"model-export/#supported-models","title":"Supported Models","text":"<p>Any PyLate-compatible ColBERT model should work. Tested models:</p> Model HuggingFace ID GTE-ModernColBERT <code>lightonai/GTE-ModernColBERT-v1</code> ColBERT-v2 <code>colbert-ir/colbertv2.0</code>"},{"location":"model-export/#using-custom-models","title":"Using Custom Models","text":"<p>Models must have:</p> <ul> <li>A compatible tokenizer</li> <li>A forward method that returns token embeddings</li> <li>Proper configuration for sequence length</li> </ul>"},{"location":"model-export/#using-exported-models","title":"Using Exported Models","text":""},{"location":"model-export/#with-nextplaid-api","title":"With NextPlaid API","text":"<pre><code># Start server with model\ndocker run -d \\\n  -p 8080:8080 \\\n  -v ./my-model-onnx:/model \\\n  ghcr.io/lightonai/next-plaid-api:latest \\\n  --model /model\n</code></pre> <p>Or from HuggingFace:</p> <pre><code>docker run -d \\\n  -p 8080:8080 \\\n  ghcr.io/lightonai/next-plaid-api:latest \\\n  --model lightonai/GTE-ModernColBERT-v1-onnx\n</code></pre>"},{"location":"model-export/#with-python-sdk","title":"With Python SDK","text":"<p>Once the server has a model loaded:</p> <pre><code>from next_plaid_client import NextPlaidClient\n\nclient = NextPlaidClient(\"http://localhost:8080\")\n\n# Encode text\nresponse = client.encode([\"Hello world\"], input_type=\"document\")\nprint(response.embeddings)\n\n# Search with text (auto-detected)\nresults = client.search(\n    \"my_index\",\n    queries=[\"What is machine learning?\"]\n)\n</code></pre>"},{"location":"model-export/#quantization","title":"Quantization","text":""},{"location":"model-export/#benefits","title":"Benefits","text":"Aspect FP32 INT8 Model size ~440 MB ~110 MB Inference speed 1x 2-3x Quality Baseline ~99% of baseline"},{"location":"model-export/#when-to-use","title":"When to Use","text":"<ul> <li>Use INT8: Production deployments, CPU inference</li> <li>Use FP32: When maximum quality is required</li> </ul>"},{"location":"model-export/#quantize-existing-model","title":"Quantize Existing Model","text":"<p>If you have an existing ONNX model:</p> <pre><code>colbert-quantize ./model.onnx -o ./model_int8.onnx\n</code></pre>"},{"location":"model-export/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model-export/#export-fails","title":"Export Fails","text":"<p>\"Model not found\"</p> <ul> <li>Check the model ID on HuggingFace</li> <li>Ensure you have network access</li> <li>For private models, set <code>HF_TOKEN</code></li> </ul> <p>\"Out of memory\"</p> <ul> <li>Reduce batch size: Some models require significant memory during export</li> <li>Use a machine with more RAM</li> </ul>"},{"location":"model-export/#model-doesnt-load-in-nextplaid","title":"Model Doesn't Load in NextPlaid","text":"<p>\"Invalid ONNX model\"</p> <ul> <li>Ensure the model was exported with the correct opset version</li> <li>Try re-exporting with the latest version of the tool</li> </ul> <p>\"Tokenizer error\"</p> <ul> <li>Ensure <code>tokenizer.json</code> is present in the model directory</li> <li>Some models may need tokenizer config adjustments</li> </ul>"},{"location":"model-export/#pre-exported-models","title":"Pre-exported Models","text":"<p>LightOn provides pre-exported models:</p> Model HuggingFace ID GTE-ModernColBERT-v1 (ONNX) <code>lightonai/GTE-ModernColBERT-v1-onnx</code> <p>Use directly without exporting:</p> <pre><code>docker run -d \\\n  -p 8080:8080 \\\n  ghcr.io/lightonai/next-plaid-api:latest \\\n  --model lightonai/GTE-ModernColBERT-v1-onnx\n</code></pre>"},{"location":"model-export/#performance-tips","title":"Performance Tips","text":""},{"location":"model-export/#onnx-runtime-optimization","title":"ONNX Runtime Optimization","text":"<p>The NextPlaid server automatically:</p> <ul> <li>Uses INT8 quantization when available</li> <li>Enables parallel sessions for throughput</li> <li>Selects optimal execution provider (CPU, CUDA)</li> </ul>"},{"location":"model-export/#hardware-acceleration","title":"Hardware Acceleration","text":"Platform Acceleration Intel CPU OpenVINO / MKL AMD CPU Default BLAS NVIDIA GPU CUDA / TensorRT Apple Silicon CoreML <p>The CUDA Docker image includes GPU support automatically.</p>"},{"location":"api/","title":"REST API","text":"<p>NextPlaid provides a RESTful HTTP API for all operations.</p>"},{"location":"api/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8080\n</code></pre>"},{"location":"api/#authentication","title":"Authentication","text":"<p>The API does not require authentication by default. For production deployments, consider using a reverse proxy (nginx, Traefik) to add authentication.</p>"},{"location":"api/#response-format","title":"Response Format","text":"<p>All responses are JSON. Successful responses return the requested data:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\"\n}\n</code></pre> <p>Error responses include a message and error code:</p> <pre><code>{\n  \"error\": \"Index not found: my_index\",\n  \"code\": \"INDEX_NOT_FOUND\"\n}\n</code></pre>"},{"location":"api/#rate-limiting","title":"Rate Limiting","text":"<p>The API implements rate limiting:</p> <ul> <li>Sustained rate: 50 requests/second</li> <li>Burst limit: 100 requests</li> </ul> <p>When rate limited, you'll receive a <code>429 Too Many Requests</code> response.</p>"},{"location":"api/#openapi-documentation","title":"OpenAPI Documentation","text":"<p>When the server is running, interactive documentation is available at:</p> <ul> <li>Swagger UI: <code>http://localhost:8080/swagger-ui/</code></li> <li>OpenAPI JSON: <code>http://localhost:8080/api-docs/openapi.json</code></li> </ul>"},{"location":"api/#endpoints-overview","title":"Endpoints Overview","text":""},{"location":"api/#health","title":"Health","text":"Method Endpoint Description GET <code>/health</code> Server health check GET <code>/</code> Root endpoint (same as health)"},{"location":"api/#indices","title":"Indices","text":"Method Endpoint Description GET <code>/indices</code> List all indices POST <code>/indices</code> Create a new index GET <code>/indices/{name}</code> Get index info DELETE <code>/indices/{name}</code> Delete an index PUT <code>/indices/{name}/config</code> Update index config"},{"location":"api/#documents","title":"Documents","text":"Method Endpoint Description POST <code>/indices/{name}/documents</code> Add documents POST <code>/indices/{name}/update</code> Update with batching POST <code>/indices/{name}/update_with_encoding</code> Add text (requires model) DELETE <code>/indices/{name}/documents</code> Delete documents"},{"location":"api/#search","title":"Search","text":"Method Endpoint Description POST <code>/indices/{name}/search</code> Search with embeddings POST <code>/indices/{name}/search/filtered</code> Filtered search POST <code>/indices/{name}/search_with_encoding</code> Text search (requires model) POST <code>/indices/{name}/search/filtered_with_encoding</code> Filtered text search"},{"location":"api/#metadata","title":"Metadata","text":"Method Endpoint Description GET <code>/indices/{name}/metadata</code> Get all metadata POST <code>/indices/{name}/metadata</code> Add metadata GET <code>/indices/{name}/metadata/count</code> Get count POST <code>/indices/{name}/metadata/check</code> Check IDs exist POST <code>/indices/{name}/metadata/query</code> Query with SQL POST <code>/indices/{name}/metadata/get</code> Get by IDs/condition"},{"location":"api/#encoding","title":"Encoding","text":"Method Endpoint Description POST <code>/encode</code> Encode text (requires model)"},{"location":"api/#health-check","title":"Health Check","text":"<p>GET <code>/health</code></p> <p>Check server health and get status information.</p> RequestResponse <pre><code>curl http://localhost:8080/health\n</code></pre> <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"loaded_indices\": 2,\n  \"index_dir\": \"/data/indices\",\n  \"memory_usage_bytes\": 524288000,\n  \"indices\": [\n    {\n      \"name\": \"my_index\",\n      \"num_documents\": 1000,\n      \"num_embeddings\": 15000,\n      \"num_partitions\": 64,\n      \"dimension\": 128,\n      \"nbits\": 4,\n      \"avg_doclen\": 15.0,\n      \"has_metadata\": true,\n      \"max_documents\": null\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#error-codes","title":"Error Codes","text":"Code HTTP Status Description <code>INDEX_NOT_FOUND</code> 404 Index does not exist <code>INDEX_EXISTS</code> 409 Index already exists <code>VALIDATION_ERROR</code> 400 Invalid request data <code>MODEL_NOT_LOADED</code> 503 Text encoding requires model <code>RATE_LIMITED</code> 429 Too many requests <code>INTERNAL_ERROR</code> 500 Server error"},{"location":"api/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>Indices - Index management endpoints</li> <li>Documents - Document operations</li> <li>Search - Search endpoints</li> <li>Metadata - Metadata operations</li> </ul>"},{"location":"api/documents/","title":"Documents API","text":"<p>Endpoints for managing documents in indices.</p>"},{"location":"api/documents/#add-documents","title":"Add Documents","text":"<p>POST <code>/indices/{name}/documents</code></p> <p>Add documents to an index. The operation is asynchronous and returns immediately.</p>"},{"location":"api/documents/#path-parameters","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/documents/#request-body","title":"Request Body","text":"Field Type Required Description <code>documents</code> <code>array</code> Yes List of documents with embeddings <code>documents[].embeddings</code> <code>array</code> Yes Token embeddings <code>[num_tokens, dim]</code> <code>metadata</code> <code>array</code> No Metadata for each document RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/documents \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"documents\": [\n      {\"embeddings\": [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]},\n      {\"embeddings\": [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2], [1.3, 1.4, 1.5]]}\n    ],\n    \"metadata\": [\n      {\"title\": \"Document 1\", \"category\": \"science\"},\n      {\"title\": \"Document 2\", \"category\": \"history\"}\n    ]\n  }'\n</code></pre> <pre><code>\"Update queued for 2 documents\"\n</code></pre>"},{"location":"api/documents/#notes","title":"Notes","text":"<ul> <li>Each document is a list of token embeddings</li> <li>All embeddings must have the same dimension</li> <li>Documents can have different numbers of tokens</li> <li>Metadata is optional but must match document count if provided</li> </ul>"},{"location":"api/documents/#errors","title":"Errors","text":"Status Code Description 404 <code>INDEX_NOT_FOUND</code> Index does not exist 400 <code>VALIDATION_ERROR</code> Invalid document format"},{"location":"api/documents/#update-documents","title":"Update Documents","text":"<p>POST <code>/indices/{name}/update</code></p> <p>Update index by adding documents with batching support. Use this for large uploads.</p>"},{"location":"api/documents/#path-parameters_1","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/documents/#request-body_1","title":"Request Body","text":"<p>Same as Add Documents.</p> RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/update \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"documents\": [\n      {\"embeddings\": [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]}\n    ],\n    \"metadata\": [\n      {\"title\": \"New Document\"}\n    ]\n  }'\n</code></pre> <pre><code>\"Update queued for 1 documents\"\n</code></pre>"},{"location":"api/documents/#update-documents-with-encoding","title":"Update Documents with Encoding","text":"<p>POST <code>/indices/{name}/update_with_encoding</code></p> <p>Add documents using text. The server encodes the text using the loaded model.</p> <p>Requires Model</p> <p>This endpoint requires the server to be started with a model loaded. See Deployment for details.</p>"},{"location":"api/documents/#path-parameters_2","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/documents/#request-body_2","title":"Request Body","text":"Field Type Required Description <code>documents</code> <code>array</code> Yes List of document texts <code>metadata</code> <code>array</code> No Metadata for each document RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/update_with_encoding \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"documents\": [\n      \"Paris is the capital of France.\",\n      \"Berlin is the capital of Germany.\"\n    ],\n    \"metadata\": [\n      {\"country\": \"France\"},\n      {\"country\": \"Germany\"}\n    ]\n  }'\n</code></pre> <pre><code>\"Update queued for 2 documents\"\n</code></pre>"},{"location":"api/documents/#errors_1","title":"Errors","text":"Status Code Description 503 <code>MODEL_NOT_LOADED</code> No model loaded 404 <code>INDEX_NOT_FOUND</code> Index does not exist"},{"location":"api/documents/#delete-documents","title":"Delete Documents","text":"<p>DELETE <code>/indices/{name}/documents</code></p> <p>Delete documents by metadata filter. The operation is asynchronous and returns immediately with HTTP 202.</p> <p>Asynchronous Operation</p> <p>This endpoint returns immediately with a 202 Accepted status. The deletion is processed in the background. Documents matching the SQL WHERE condition will be deleted from both the index and the metadata database.</p>"},{"location":"api/documents/#path-parameters_3","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/documents/#request-body_3","title":"Request Body","text":"Field Type Required Description <code>condition</code> <code>string</code> Yes SQL WHERE condition for selecting documents to delete <code>parameters</code> <code>array</code> No Parameters for the condition (default: <code>[]</code>)"},{"location":"api/documents/#sql-condition-examples","title":"SQL Condition Examples","text":"Condition Parameters Description <code>category = ?</code> <code>[\"science\"]</code> Delete all documents in the \"science\" category <code>year &lt; ?</code> <code>[2020]</code> Delete documents older than 2020 <code>category = ? AND year &lt; ?</code> <code>[\"outdated\", 2020]</code> Combine multiple conditions <code>status IN (?, ?)</code> <code>[\"archived\", \"deleted\"]</code> Delete documents with multiple status values RequestResponse <pre><code>curl -X DELETE http://localhost:8080/indices/my_index/documents \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"condition\": \"category = ? AND year &lt; ?\",\n    \"parameters\": [\"outdated\", 2020]\n  }'\n</code></pre> <pre><code>\"Delete queued: 15 documents matching condition\"\n</code></pre>"},{"location":"api/documents/#response","title":"Response","text":"<p>The endpoint returns a string message indicating how many documents matched the condition and were queued for deletion.</p>"},{"location":"api/documents/#errors_2","title":"Errors","text":"Status Code Description 400 <code>BAD_REQUEST</code> Empty condition or invalid SQL syntax 404 <code>INDEX_NOT_FOUND</code> Index does not exist 404 <code>METADATA_NOT_FOUND</code> Metadata database not found for this index 503 <code>SERVICE_UNAVAILABLE</code> Delete queue full, retry later"},{"location":"api/documents/#embedding-format","title":"Embedding Format","text":"<p>Documents are represented as multi-vector embeddings:</p> <pre><code>{\n  \"embeddings\": [\n    [0.1, 0.2, 0.3, ...],  // Token 1 (dim dimensions)\n    [0.4, 0.5, 0.6, ...],  // Token 2\n    [0.7, 0.8, 0.9, ...]   // Token 3\n  ]\n}\n</code></pre> <ul> <li>Each document has a variable number of tokens</li> <li>All embeddings must have the same dimension (e.g., 128)</li> <li>The dimension is determined by the first document added to the index</li> </ul>"},{"location":"api/documents/#generating-embeddings","title":"Generating Embeddings","text":"<p>Embeddings are typically generated using a ColBERT model:</p> <pre><code>from pylate import ColBERT\n\nmodel = ColBERT(\"lightonai/GTE-ModernColBERT-v1\")\n\n# Encode documents\ndocuments = [\"Document 1 text\", \"Document 2 text\"]\nembeddings = model.encode(documents)\n\n# Each embedding is [num_tokens, dim]\n# embeddings[0].shape = (15, 128)  # 15 tokens, 128 dimensions\n</code></pre> <p>Or use the API's text encoding endpoint if a model is loaded.</p>"},{"location":"api/indices/","title":"Indices API","text":"<p>Endpoints for managing indices.</p>"},{"location":"api/indices/#list-indices","title":"List Indices","text":"<p>GET <code>/indices</code></p> <p>List all available indices.</p> RequestResponse <pre><code>curl http://localhost:8080/indices\n</code></pre> <pre><code>[\"index1\", \"index2\", \"my_documents\"]\n</code></pre>"},{"location":"api/indices/#create-index","title":"Create Index","text":"<p>POST <code>/indices</code></p> <p>Create a new index with the specified configuration.</p>"},{"location":"api/indices/#request-body","title":"Request Body","text":"Field Type Required Description <code>name</code> <code>string</code> Yes Index name <code>config</code> <code>object</code> No Index configuration <code>config.nbits</code> <code>int</code> No Quantization bits (2 or 4, default: 4) <code>config.batch_size</code> <code>int</code> No Indexing batch size (default: 50000) <code>config.seed</code> <code>int</code> No Random seed <code>config.start_from_scratch</code> <code>int</code> No Rebuild threshold (default: 999) <code>config.max_documents</code> <code>int</code> No Maximum documents RequestResponse <pre><code>curl -X POST http://localhost:8080/indices \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"my_index\",\n    \"config\": {\n      \"nbits\": 4,\n      \"batch_size\": 50000,\n      \"max_documents\": 100000\n    }\n  }'\n</code></pre> <pre><code>{\n  \"message\": \"Index created\",\n  \"name\": \"my_index\"\n}\n</code></pre>"},{"location":"api/indices/#errors","title":"Errors","text":"Status Code Description 409 <code>INDEX_EXISTS</code> Index already exists 400 <code>VALIDATION_ERROR</code> Invalid configuration"},{"location":"api/indices/#get-index","title":"Get Index","text":"<p>GET <code>/indices/{name}</code></p> <p>Get detailed information about an index.</p>"},{"location":"api/indices/#path-parameters","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name RequestResponse <pre><code>curl http://localhost:8080/indices/my_index\n</code></pre> <pre><code>{\n  \"name\": \"my_index\",\n  \"num_documents\": 1000,\n  \"num_embeddings\": 15000,\n  \"num_partitions\": 64,\n  \"avg_doclen\": 15.0,\n  \"dimension\": 128,\n  \"has_metadata\": true,\n  \"metadata_count\": 1000,\n  \"max_documents\": 100000\n}\n</code></pre>"},{"location":"api/indices/#response-fields","title":"Response Fields","text":"Field Type Description <code>name</code> <code>string</code> Index name <code>num_documents</code> <code>int</code> Number of indexed documents <code>num_embeddings</code> <code>int</code> Total token embeddings <code>num_partitions</code> <code>int</code> Number of IVF partitions <code>avg_doclen</code> <code>float</code> Average tokens per document <code>dimension</code> <code>int</code> Embedding dimension <code>has_metadata</code> <code>bool</code> Whether metadata exists <code>metadata_count</code> <code>int?</code> Number of metadata entries <code>max_documents</code> <code>int?</code> Maximum documents limit"},{"location":"api/indices/#errors_1","title":"Errors","text":"Status Code Description 404 <code>INDEX_NOT_FOUND</code> Index does not exist"},{"location":"api/indices/#delete-index","title":"Delete Index","text":"<p>DELETE <code>/indices/{name}</code></p> <p>Delete an index and all its data.</p> <p>Irreversible</p> <p>This operation permanently deletes all documents and metadata. It cannot be undone.</p>"},{"location":"api/indices/#path-parameters_1","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name RequestResponse <pre><code>curl -X DELETE http://localhost:8080/indices/my_index\n</code></pre> <pre><code>{\n  \"message\": \"Index deleted\",\n  \"name\": \"my_index\"\n}\n</code></pre>"},{"location":"api/indices/#errors_2","title":"Errors","text":"Status Code Description 404 <code>INDEX_NOT_FOUND</code> Index does not exist"},{"location":"api/indices/#update-index-config","title":"Update Index Config","text":"<p>PUT <code>/indices/{name}/config</code></p> <p>Update index configuration settings.</p>"},{"location":"api/indices/#path-parameters_2","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/indices/#request-body_1","title":"Request Body","text":"Field Type Required Description <code>max_documents</code> <code>int?</code> No New max documents limit (null to remove) RequestResponse <pre><code>curl -X PUT http://localhost:8080/indices/my_index/config \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"max_documents\": 50000}'\n</code></pre> <pre><code>{\n  \"message\": \"Index config updated\",\n  \"max_documents\": 50000\n}\n</code></pre>"},{"location":"api/indices/#errors_3","title":"Errors","text":"Status Code Description 404 <code>INDEX_NOT_FOUND</code> Index does not exist"},{"location":"api/metadata/","title":"Metadata API","text":"<p>Endpoints for managing document metadata. Metadata is stored in SQLite and enables filtered search.</p>"},{"location":"api/metadata/#get-all-metadata","title":"Get All Metadata","text":"<p>GET <code>/indices/{name}/metadata</code></p> <p>Get all metadata entries for an index.</p>"},{"location":"api/metadata/#path-parameters","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name RequestResponse <pre><code>curl http://localhost:8080/indices/my_index/metadata\n</code></pre> <pre><code>{\n  \"metadata\": [\n    {\"_id\": 0, \"title\": \"Document 1\", \"category\": \"science\"},\n    {\"_id\": 1, \"title\": \"Document 2\", \"category\": \"history\"},\n    {\"_id\": 2, \"title\": \"Document 3\", \"category\": \"science\"}\n  ],\n  \"count\": 3\n}\n</code></pre>"},{"location":"api/metadata/#response-fields","title":"Response Fields","text":"Field Type Description <code>metadata</code> <code>array</code> List of metadata entries <code>count</code> <code>int</code> Total number of entries <p>Reserved Field</p> <p>The <code>_id</code> field is automatically added and corresponds to the document ID.</p>"},{"location":"api/metadata/#add-metadata","title":"Add Metadata","text":"<p>POST <code>/indices/{name}/metadata</code></p> <p>Add or update metadata entries.</p>"},{"location":"api/metadata/#path-parameters_1","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/metadata/#request-body","title":"Request Body","text":"Field Type Required Description <code>metadata</code> <code>array</code> Yes List of metadata objects RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/metadata \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"metadata\": [\n      {\"_id\": 0, \"title\": \"Updated Title\", \"category\": \"science\"},\n      {\"_id\": 1, \"title\": \"New Document\", \"year\": 2024}\n    ]\n  }'\n</code></pre> <pre><code>{\n  \"added\": 2\n}\n</code></pre>"},{"location":"api/metadata/#notes","title":"Notes","text":"<ul> <li>Include <code>_id</code> to update existing metadata</li> <li>Omit <code>_id</code> when adding new metadata (auto-assigned based on document order)</li> <li>Metadata is typically added with documents via <code>add</code></li> </ul>"},{"location":"api/metadata/#get-metadata-count","title":"Get Metadata Count","text":"<p>GET <code>/indices/{name}/metadata/count</code></p> <p>Get the count of metadata entries.</p>"},{"location":"api/metadata/#path-parameters_2","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name RequestResponse <pre><code>curl http://localhost:8080/indices/my_index/metadata/count\n</code></pre> <pre><code>{\n  \"count\": 1000,\n  \"has_metadata\": true\n}\n</code></pre>"},{"location":"api/metadata/#check-metadata-exists","title":"Check Metadata Exists","text":"<p>POST <code>/indices/{name}/metadata/check</code></p> <p>Check which document IDs have metadata entries.</p>"},{"location":"api/metadata/#path-parameters_3","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/metadata/#request-body_1","title":"Request Body","text":"Field Type Required Description <code>document_ids</code> <code>array</code> Yes List of document IDs to check RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/metadata/check \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"document_ids\": [0, 1, 2, 999, 1000]}'\n</code></pre> <pre><code>{\n  \"existing_ids\": [0, 1, 2],\n  \"missing_ids\": [999, 1000],\n  \"existing_count\": 3,\n  \"missing_count\": 2\n}\n</code></pre>"},{"location":"api/metadata/#query-metadata","title":"Query Metadata","text":"<p>POST <code>/indices/{name}/metadata/query</code></p> <p>Query metadata using SQL WHERE conditions. Returns matching document IDs.</p>"},{"location":"api/metadata/#path-parameters_4","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/metadata/#request-body_2","title":"Request Body","text":"Field Type Required Description <code>condition</code> <code>string</code> Yes SQL WHERE condition <code>parameters</code> <code>array</code> No Parameters for placeholders RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/metadata/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"condition\": \"category = ? AND year &gt;= ?\",\n    \"parameters\": [\"science\", 2020]\n  }'\n</code></pre> <pre><code>{\n  \"document_ids\": [0, 2, 5, 8, 12],\n  \"count\": 5\n}\n</code></pre>"},{"location":"api/metadata/#filter-syntax","title":"Filter Syntax","text":"<p>The condition uses SQLite syntax:</p> <pre><code>-- Simple equality\ncategory = ?\n\n-- Multiple conditions\ncategory = ? AND year &gt;= ?\n\n-- OR conditions\ncategory = ? OR category = ?\n\n-- IN clause (use multiple ? placeholders)\ncategory IN (?, ?, ?)\n\n-- LIKE for pattern matching\ntitle LIKE ?  -- Use \"%\" as wildcard in parameter\n\n-- NULL checks\ndescription IS NOT NULL\n\n-- Comparison operators\nyear &gt; ? AND year &lt; ?\nscore &gt;= ?\n</code></pre>"},{"location":"api/metadata/#example-queries","title":"Example Queries","text":"<p>Category filter:</p> <pre><code>{\n  \"condition\": \"category = ?\",\n  \"parameters\": [\"science\"]\n}\n</code></pre> <p>Year range:</p> <pre><code>{\n  \"condition\": \"year &gt;= ? AND year &lt;= ?\",\n  \"parameters\": [2020, 2024]\n}\n</code></pre> <p>Text search:</p> <pre><code>{\n  \"condition\": \"title LIKE ?\",\n  \"parameters\": [\"%machine learning%\"]\n}\n</code></pre> <p>Multiple categories:</p> <pre><code>{\n  \"condition\": \"category IN (?, ?, ?)\",\n  \"parameters\": [\"science\", \"technology\", \"engineering\"]\n}\n</code></pre>"},{"location":"api/metadata/#get-metadata-by-ids","title":"Get Metadata by IDs","text":"<p>POST <code>/indices/{name}/metadata/get</code></p> <p>Get metadata for specific document IDs or by SQL condition.</p>"},{"location":"api/metadata/#path-parameters_5","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/metadata/#request-body_3","title":"Request Body","text":"Field Type Required Description <code>document_ids</code> <code>array</code> No Specific document IDs <code>condition</code> <code>string</code> No SQL WHERE condition <code>parameters</code> <code>array</code> No Parameters for condition <code>limit</code> <code>int</code> No Maximum results <p>Provide either <code>document_ids</code> or <code>condition</code>, not both.</p> By IDsBy ConditionResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/metadata/get \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"document_ids\": [0, 1, 2]}'\n</code></pre> <pre><code>curl -X POST http://localhost:8080/indices/my_index/metadata/get \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"condition\": \"category = ?\",\n    \"parameters\": [\"science\"],\n    \"limit\": 10\n  }'\n</code></pre> <pre><code>{\n  \"metadata\": [\n    {\"_id\": 0, \"title\": \"Document 1\", \"category\": \"science\"},\n    {\"_id\": 2, \"title\": \"Document 3\", \"category\": \"science\"}\n  ],\n  \"count\": 2\n}\n</code></pre>"},{"location":"api/metadata/#metadata-schema","title":"Metadata Schema","text":"<p>Metadata is stored as JSON in SQLite. Any valid JSON structure is supported:</p> <pre><code>{\n  \"_id\": 0,\n  \"title\": \"Document Title\",\n  \"category\": \"science\",\n  \"tags\": [\"ml\", \"nlp\", \"search\"],\n  \"year\": 2024,\n  \"score\": 0.95,\n  \"published\": true,\n  \"author\": {\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\"\n  }\n}\n</code></pre>"},{"location":"api/metadata/#queryable-fields","title":"Queryable Fields","text":"<p>Only top-level scalar fields can be queried:</p> <ul> <li><code>title</code>, <code>category</code>, <code>year</code>, <code>score</code>, <code>published</code></li> </ul> <p>Nested objects and arrays are stored but not directly queryable.</p>"},{"location":"api/metadata/#best-practices","title":"Best Practices","text":"<ol> <li>Keep metadata flat - Nested structures can't be filtered</li> <li>Use consistent types - Same field should have same type across documents</li> <li>Index important fields - Frequently filtered fields benefit from structure</li> <li>Limit size - Large metadata increases memory usage</li> </ol>"},{"location":"api/search/","title":"Search API","text":"<p>Endpoints for searching indices.</p>"},{"location":"api/search/#search","title":"Search","text":"<p>POST <code>/indices/{name}/search</code></p> <p>Search an index with query embeddings.</p>"},{"location":"api/search/#path-parameters","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/search/#request-body","title":"Request Body","text":"Field Type Required Description <code>queries</code> <code>array</code> Yes List of query embeddings <code>params</code> <code>object</code> No Search parameters <code>params.top_k</code> <code>int</code> No Results per query (default: 10) <code>params.n_ivf_probe</code> <code>int</code> No IVF cells to probe (default: 8) <code>params.n_full_scores</code> <code>int</code> No Candidates for scoring (default: 4096) <code>params.centroid_score_threshold</code> <code>float?</code> No Centroid pruning threshold (default: 0.4). Set to null to disable <code>subset</code> <code>array</code> No Limit search to these document IDs RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"queries\": [\n      [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n    ],\n    \"params\": {\n      \"top_k\": 10,\n      \"n_ivf_probe\": 8\n    }\n  }'\n</code></pre> <pre><code>{\n  \"results\": [\n    {\n      \"query_id\": 0,\n      \"document_ids\": [42, 17, 89, 3, 56],\n      \"scores\": [0.95, 0.87, 0.82, 0.78, 0.71],\n      \"metadata\": [\n        {\"title\": \"Document 42\"},\n        {\"title\": \"Document 17\"},\n        {\"title\": \"Document 89\"},\n        {\"title\": \"Document 3\"},\n        {\"title\": \"Document 56\"}\n      ]\n    }\n  ],\n  \"num_queries\": 1\n}\n</code></pre>"},{"location":"api/search/#response-fields","title":"Response Fields","text":"Field Type Description <code>results</code> <code>array</code> Results for each query <code>results[].query_id</code> <code>int</code> Query index <code>results[].document_ids</code> <code>array</code> Matching document IDs <code>results[].scores</code> <code>array</code> Relevance scores (higher = better) <code>results[].metadata</code> <code>array?</code> Document metadata (if available) <code>num_queries</code> <code>int</code> Number of queries processed"},{"location":"api/search/#errors","title":"Errors","text":"Status Code Description 404 <code>INDEX_NOT_FOUND</code> Index does not exist"},{"location":"api/search/#filtered-search","title":"Filtered Search","text":"<p>POST <code>/indices/{name}/search/filtered</code></p> <p>Search with metadata filtering using SQL-like conditions.</p>"},{"location":"api/search/#path-parameters_1","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/search/#request-body_1","title":"Request Body","text":"Field Type Required Description <code>queries</code> <code>array</code> Yes List of query embeddings <code>filter_condition</code> <code>string</code> Yes SQL WHERE condition <code>filter_parameters</code> <code>array</code> No Parameters for placeholders <code>params</code> <code>object</code> No Search parameters RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/search/filtered \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"queries\": [\n      [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n    ],\n    \"filter_condition\": \"category = ? AND year &gt;= ?\",\n    \"filter_parameters\": [\"science\", 2020],\n    \"params\": {\"top_k\": 10}\n  }'\n</code></pre> <pre><code>{\n  \"results\": [\n    {\n      \"query_id\": 0,\n      \"document_ids\": [42, 89],\n      \"scores\": [0.95, 0.82],\n      \"metadata\": [\n        {\"title\": \"Document 42\", \"category\": \"science\", \"year\": 2023},\n        {\"title\": \"Document 89\", \"category\": \"science\", \"year\": 2022}\n      ]\n    }\n  ],\n  \"num_queries\": 1\n}\n</code></pre>"},{"location":"api/search/#filter-syntax","title":"Filter Syntax","text":"<p>The filter uses SQLite syntax with <code>?</code> placeholders:</p> <pre><code>-- Equality\ncategory = ?\n\n-- Comparison\nyear &gt;= ? AND year &lt;= ?\n\n-- Multiple conditions\ncategory = ? AND (year &gt;= ? OR featured = ?)\n\n-- Text matching\ntitle LIKE ?\n\n-- NULL checks\ndescription IS NOT NULL\n\n-- IN clause\ncategory IN (?, ?, ?)\n</code></pre>"},{"location":"api/search/#search-with-encoding","title":"Search with Encoding","text":"<p>POST <code>/indices/{name}/search_with_encoding</code></p> <p>Search using text queries. The server encodes queries using the loaded model.</p> <p>Requires Model</p> <p>This endpoint requires the server to be started with a model loaded.</p>"},{"location":"api/search/#path-parameters_2","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/search/#request-body_2","title":"Request Body","text":"Field Type Required Description <code>queries</code> <code>array</code> Yes List of text queries <code>params</code> <code>object</code> No Search parameters <code>subset</code> <code>array</code> No Limit to these document IDs RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/search_with_encoding \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"queries\": [\"What is the capital of France?\"],\n    \"params\": {\"top_k\": 5}\n  }'\n</code></pre> <pre><code>{\n  \"results\": [\n    {\n      \"query_id\": 0,\n      \"document_ids\": [0, 15, 42],\n      \"scores\": [0.92, 0.78, 0.65],\n      \"metadata\": [\n        {\"title\": \"Paris\", \"country\": \"France\"},\n        {\"title\": \"French Cities\", \"country\": \"France\"},\n        {\"title\": \"European Capitals\", \"country\": null}\n      ]\n    }\n  ],\n  \"num_queries\": 1\n}\n</code></pre>"},{"location":"api/search/#errors_1","title":"Errors","text":"Status Code Description 503 <code>MODEL_NOT_LOADED</code> No model loaded 404 <code>INDEX_NOT_FOUND</code> Index does not exist"},{"location":"api/search/#filtered-search-with-encoding","title":"Filtered Search with Encoding","text":"<p>POST <code>/indices/{name}/search/filtered_with_encoding</code></p> <p>Search with text queries and metadata filtering.</p> <p>Requires Model</p> <p>This endpoint requires the server to be started with a model loaded.</p>"},{"location":"api/search/#path-parameters_3","title":"Path Parameters","text":"Parameter Type Description <code>name</code> <code>string</code> Index name"},{"location":"api/search/#request-body_3","title":"Request Body","text":"Field Type Required Description <code>queries</code> <code>array</code> Yes List of text queries <code>filter_condition</code> <code>string</code> Yes SQL WHERE condition <code>filter_parameters</code> <code>array</code> No Parameters for placeholders <code>params</code> <code>object</code> No Search parameters RequestResponse <pre><code>curl -X POST http://localhost:8080/indices/my_index/search/filtered_with_encoding \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"queries\": [\"Famous landmarks\"],\n    \"filter_condition\": \"country = ?\",\n    \"filter_parameters\": [\"France\"],\n    \"params\": {\"top_k\": 5}\n  }'\n</code></pre> <pre><code>{\n  \"results\": [\n    {\n      \"query_id\": 0,\n      \"document_ids\": [0, 3],\n      \"scores\": [0.89, 0.75],\n      \"metadata\": [\n        {\"title\": \"Eiffel Tower\", \"country\": \"France\"},\n        {\"title\": \"Louvre Museum\", \"country\": \"France\"}\n      ]\n    }\n  ],\n  \"num_queries\": 1\n}\n</code></pre>"},{"location":"api/search/#search-parameters","title":"Search Parameters","text":"Parameter Default Description <code>top_k</code> 10 Number of results to return per query <code>n_ivf_probe</code> 8 Number of IVF partitions to search <code>n_full_scores</code> 4096 Candidates for exact scoring <code>batch_size</code> 2000 Documents per scoring batch <code>centroid_batch_size</code> 100000 Batch size for centroid scoring (0 = exhaustive) <code>centroid_score_threshold</code> 0.4 Centroid pruning threshold. Set to null to disable"},{"location":"api/search/#centroid-score-threshold","title":"Centroid Score Threshold","text":"<p>The <code>centroid_score_threshold</code> parameter enables centroid pruning during search. Centroids with a maximum score (across all query tokens) below this threshold are filtered out before scoring. This significantly speeds up search with minimal quality impact.</p> <ul> <li>Default (0.4): Good balance between speed and quality</li> <li>Higher values (0.45-0.5): Faster, more aggressive pruning (use for smaller k values)</li> <li>Lower values (0.3-0.4): More candidates, better recall (use for larger k values)</li> <li>null: Disable pruning entirely (slowest but most accurate)</li> </ul>"},{"location":"api/search/#batch-search","title":"Batch Search","text":"<p>You can search with multiple queries in a single request:</p> <pre><code>curl -X POST http://localhost:8080/indices/my_index/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"queries\": [\n      [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],\n      [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]],\n      [[1.3, 1.4, 1.5], [1.6, 1.7, 1.8]]\n    ],\n    \"params\": {\"top_k\": 10}\n  }'\n</code></pre> <p>The response will contain results for each query:</p> <pre><code>{\n  \"results\": [\n    {\"query_id\": 0, \"document_ids\": [...], \"scores\": [...]},\n    {\"query_id\": 1, \"document_ids\": [...], \"scores\": [...]},\n    {\"query_id\": 2, \"document_ids\": [...], \"scores\": [...]}\n  ],\n  \"num_queries\": 3\n}\n</code></pre>"},{"location":"concepts/multi-vector/","title":"Multi-Vector Search","text":"<p>Understanding how multi-vector search differs from traditional vector search.</p>"},{"location":"concepts/multi-vector/#traditional-vector-search","title":"Traditional Vector Search","text":"<p>In traditional dense retrieval, each document and query is represented as a single vector:</p> <pre><code>Document: \"Paris is the capital of France\" \u2192 [0.1, 0.2, ..., 0.5]  (1 x dim)\nQuery:    \"What is France's capital?\"      \u2192 [0.3, 0.4, ..., 0.6]  (1 x dim)\n</code></pre> <p>Similarity is computed using cosine similarity or dot product between these single vectors.</p>"},{"location":"concepts/multi-vector/#limitations","title":"Limitations","text":"<ul> <li>Information loss: Compressing an entire document into one vector loses nuance</li> <li>Length bias: Long documents may be poorly represented</li> <li>Semantic dilution: Key terms get averaged with less relevant content</li> </ul>"},{"location":"concepts/multi-vector/#multi-vector-search_1","title":"Multi-Vector Search","text":"<p>Multi-vector search represents documents and queries as multiple vectors - typically one per token:</p> <pre><code>Document: \"Paris is the capital of France\"\n          \u2192 [[v_Paris], [v_is], [v_the], [v_capital], [v_of], [v_France]]\n          (6 x dim)\n\nQuery:    \"What is France's capital?\"\n          \u2192 [[v_What], [v_is], [v_France's], [v_capital]]\n          (4 x dim)\n</code></pre>"},{"location":"concepts/multi-vector/#maxsim-scoring","title":"MaxSim Scoring","text":"<p>Relevance is computed using MaxSim (Maximum Similarity):</p> <ol> <li>For each query token, find its maximum similarity to any document token</li> <li>Sum these maximum similarities</li> </ol> <pre><code>score = \u03a3 max(sim(q_i, d_j) for all j in document)\n        for all i in query\n</code></pre> <p>This allows fine-grained matching where:</p> <ul> <li>\"France\" in the query matches \"France\" in the document strongly</li> <li>\"capital\" in the query matches \"capital\" in the document strongly</li> <li>Less relevant query terms contribute less to the final score</li> </ul>"},{"location":"concepts/multi-vector/#colbert-architecture","title":"ColBERT Architecture","text":"<p>NextPlaid uses embeddings from ColBERT (Contextualized Late Interaction over BERT) models.</p>"},{"location":"concepts/multi-vector/#how-colbert-works","title":"How ColBERT Works","text":"<ol> <li>Encoding: BERT produces contextualized embeddings for each token</li> <li>Projection: Embeddings are projected to a lower dimension (e.g., 128)</li> <li>Normalization: Vectors are L2-normalized</li> </ol> <pre><code># Simplified ColBERT encoding\ndef encode(text):\n    tokens = tokenize(text)\n    bert_embeddings = bert(tokens)        # [num_tokens, 768]\n    projected = linear(bert_embeddings)    # [num_tokens, 128]\n    normalized = l2_normalize(projected)   # [num_tokens, 128]\n    return normalized\n</code></pre>"},{"location":"concepts/multi-vector/#late-interaction","title":"Late Interaction","text":"<p>\"Late interaction\" means the query and document are encoded independently, then interact only at scoring time:</p> <pre><code>Query encoding  \u2500\u2500\u2500\u2510\n                   \u251c\u2500\u2500\u2192 MaxSim scoring\nDocument encoding \u2500\u2518\n</code></pre> <p>This enables:</p> <ul> <li>Pre-computed document embeddings: Encode once, search many times</li> <li>Efficient indexing: Documents can be indexed offline</li> <li>Scalable search: Only queries need real-time encoding</li> </ul>"},{"location":"concepts/multi-vector/#comparison","title":"Comparison","text":"Aspect Single-Vector Multi-Vector Representation 1 vector per document N vectors per document Storage ~128 floats/doc ~128 \u00d7 N floats/doc Scoring Dot product MaxSim Accuracy Good Excellent Speed Very fast Fast (with PLAID)"},{"location":"concepts/multi-vector/#when-to-use-multi-vector","title":"When to Use Multi-Vector","text":"<p>Choose multi-vector (ColBERT/NextPlaid) when:</p> <ul> <li>Accuracy is critical</li> <li>Documents vary in length</li> <li>Fine-grained matching matters</li> <li>You have the storage budget</li> </ul> <p>Choose single-vector when:</p> <ul> <li>Latency is extremely critical</li> <li>Storage is very constrained</li> <li>Good enough accuracy is acceptable</li> </ul>"},{"location":"concepts/multi-vector/#practical-example","title":"Practical Example","text":"<p>Consider searching for \"machine learning applications in healthcare\":</p>"},{"location":"concepts/multi-vector/#single-vector","title":"Single-Vector","text":"<p>The entire query becomes one vector. It might match documents about:</p> <ul> <li>Machine learning (partial match)</li> <li>Healthcare (partial match)</li> <li>Generic AI documents (semantic similarity)</li> </ul>"},{"location":"concepts/multi-vector/#multi-vector","title":"Multi-Vector","text":"<p>Each concept gets its own vector:</p> Query Token Best Document Match Score machine \"machine\" 0.95 learning \"learning\" 0.92 applications \"applications\" 0.88 healthcare \"medical\" 0.85 <p>Documents must match multiple concepts well to score highly.</p>"},{"location":"concepts/multi-vector/#storage-considerations","title":"Storage Considerations","text":"<p>Multi-vector search requires more storage:</p> <pre><code>Single-vector:  1,000,000 docs \u00d7 128 dims \u00d7 4 bytes = 512 MB\nMulti-vector:   1,000,000 docs \u00d7 15 tokens \u00d7 128 dims \u00d7 4 bytes = 7.7 GB\n</code></pre>"},{"location":"concepts/multi-vector/#how-nextplaid-reduces-storage","title":"How NextPlaid Reduces Storage","text":"<p>NextPlaid uses the PLAID algorithm to compress embeddings:</p> <ol> <li>Quantization: 2-bit or 4-bit storage instead of 32-bit floats</li> <li>Residual encoding: Store difference from cluster centroids</li> <li>IVF indexing: Only load relevant partitions</li> </ol> <p>Result: ~90% storage reduction compared to naive multi-vector storage.</p>"},{"location":"concepts/multi-vector/#further-reading","title":"Further Reading","text":"<ul> <li>PLAID Algorithm - How NextPlaid achieves efficient multi-vector search</li> <li>ColBERT Paper - Original ColBERT research</li> <li>ColBERTv2 Paper - Improved efficiency</li> </ul>"},{"location":"concepts/plaid/","title":"PLAID Algorithm","text":"<p>Understanding how NextPlaid achieves efficient multi-vector search.</p>"},{"location":"concepts/plaid/#overview","title":"Overview","text":"<p>PLAID (Performance-optimized Late Interaction using Approximate Deferred scoring) is an algorithm that makes multi-vector search practical at scale.</p> <p>The key insight: instead of computing exact MaxSim scores for all documents, use a multi-stage pipeline that progressively narrows candidates.</p>"},{"location":"concepts/plaid/#the-challenge","title":"The Challenge","text":"<p>Naive multi-vector search is expensive:</p> <pre><code>1M documents \u00d7 15 tokens/doc \u00d7 10 query tokens \u00d7 128 dims\n= 19.2 billion operations per query\n</code></pre> <p>This is impractical for real-time search.</p>"},{"location":"concepts/plaid/#plaid-pipeline","title":"PLAID Pipeline","text":"<p>PLAID uses a 4-stage pipeline:</p> <pre><code>Query\n  \u2193\n1. IVF Probing (find relevant partitions)\n  \u2193\n2. Approximate Scoring (using centroids)\n  \u2193\n3. Pruning (keep top candidates)\n  \u2193\n4. Exact Scoring (decompress and score)\n  \u2193\nResults\n</code></pre>"},{"location":"concepts/plaid/#stage-1-ivf-probing","title":"Stage 1: IVF Probing","text":"<p>Goal: Find which index partitions contain relevant documents.</p> <ol> <li>Compute query-centroid similarities</li> <li>Select top-k centroids per query token</li> <li>Retrieve documents from those partitions</li> </ol> <pre><code>Query tokens \u2192 Centroid scores \u2192 Top partitions \u2192 Candidate docs\n</code></pre> <p>Parameter: <code>n_ivf_probe</code> (default: 8)</p>"},{"location":"concepts/plaid/#stage-2-approximate-scoring","title":"Stage 2: Approximate Scoring","text":"<p>Goal: Quickly estimate document relevance.</p> <p>Instead of using full embeddings, use precomputed centroid scores:</p> <pre><code>score_approx = \u03a3 centroid_score[doc_token_centroid]\n               for each query token\n</code></pre> <p>This is fast because:</p> <ul> <li>Centroid assignments are stored as integers (codes)</li> <li>Centroid scores are computed once per query</li> </ul>"},{"location":"concepts/plaid/#stage-3-pruning","title":"Stage 3: Pruning","text":"<p>Goal: Reduce candidates to a manageable set.</p> <p>Keep only the top <code>n_full_scores</code> documents by approximate score.</p> <p>Parameter: <code>n_full_scores</code> (default: 4096)</p>"},{"location":"concepts/plaid/#stage-4-exact-scoring","title":"Stage 4: Exact Scoring","text":"<p>Goal: Compute precise MaxSim for final ranking.</p> <ol> <li>Decompress embeddings for candidate documents</li> <li>Compute exact token-level similarities</li> <li>Apply MaxSim scoring</li> <li>Return top-k results</li> </ol>"},{"location":"concepts/plaid/#index-structure","title":"Index Structure","text":""},{"location":"concepts/plaid/#centroids","title":"Centroids","text":"<p>K-means clustering groups similar token embeddings:</p> <pre><code>All token embeddings \u2192 K-means \u2192 K centroids\n</code></pre> <p>Each centroid represents a cluster of semantically similar tokens.</p> <p>Default: K \u2248 sqrt(num_embeddings)</p>"},{"location":"concepts/plaid/#codes","title":"Codes","text":"<p>For each document token, store which centroid it belongs to:</p> <pre><code>Document: [token1, token2, token3]\nCodes:    [42,     17,     89]     # Centroid indices\n</code></pre> <p>Codes are stored as 16-bit integers.</p>"},{"location":"concepts/plaid/#residuals","title":"Residuals","text":"<p>Store the difference from the centroid (quantized):</p> <pre><code>residual = token_embedding - centroid[code]\nquantized_residual = quantize(residual, nbits)\n</code></pre> <p>Options:</p> <ul> <li>4-bit (default): 16 levels per dimension</li> <li>2-bit: 4 levels per dimension (faster, less accurate)</li> </ul>"},{"location":"concepts/plaid/#inverted-file-ivf","title":"Inverted File (IVF)","text":"<p>Maps centroids to documents:</p> <pre><code>Centroid 0 \u2192 [doc_3, doc_17, doc_42, ...]\nCentroid 1 \u2192 [doc_1, doc_8, doc_99, ...]\n...\n</code></pre> <p>Enables fast lookup of documents containing tokens near a query token.</p>"},{"location":"concepts/plaid/#configuration","title":"Configuration","text":""},{"location":"concepts/plaid/#index-configuration","title":"Index Configuration","text":"Parameter Default Description <code>nbits</code> 4 Quantization bits (2 or 4) <code>batch_size</code> 50000 Tokens per indexing batch"},{"location":"concepts/plaid/#search-parameters","title":"Search Parameters","text":"Parameter Default Description <code>n_ivf_probe</code> 8 Partitions to search <code>n_full_scores</code> 4096 Candidates for exact scoring <code>top_k</code> 10 Results to return"},{"location":"concepts/plaid/#compression-analysis","title":"Compression Analysis","text":""},{"location":"concepts/plaid/#storage-breakdown","title":"Storage Breakdown","text":"<p>For 1M documents with 15 tokens/doc and 128 dimensions:</p> Component Size Description Codes 30 MB 1M \u00d7 15 \u00d7 2 bytes Residuals (4-bit) 960 MB 1M \u00d7 15 \u00d7 128 \u00d7 0.5 bytes Centroids 32 MB 16K \u00d7 128 \u00d7 4 bytes IVF ~20 MB Inverted lists Total ~1 GB <p>Compare to uncompressed: 1M \u00d7 15 \u00d7 128 \u00d7 4 bytes = 7.7 GB</p> <p>Compression ratio: ~8x</p>"},{"location":"concepts/plaid/#memory-mapping","title":"Memory Mapping","text":"<p>NextPlaid supports memory-mapped indices:</p> <pre><code>let index = MmapIndex::load(\"path/to/index\")?;\n</code></pre> <p>Benefits:</p> <ul> <li>Index larger than RAM</li> <li>Shared between processes</li> <li>Fast startup (no loading)</li> </ul>"},{"location":"concepts/plaid/#decompression","title":"Decompression","text":"<p>When exact scoring is needed, embeddings are reconstructed:</p> <pre><code>embedding = centroid[code] + dequantize(residual)\n</code></pre> <p>The reconstruction error depends on:</p> <ul> <li>Number of centroids (more = better)</li> <li>Quantization bits (4 &gt; 2)</li> </ul>"},{"location":"concepts/plaid/#quality-impact","title":"Quality Impact","text":"<p>On SciFact benchmark:</p> Configuration MAP Score vs. Uncompressed 4-bit 0.708 -0.5% 2-bit 0.695 -2.3% Uncompressed 0.712 baseline"},{"location":"concepts/plaid/#incremental-updates","title":"Incremental Updates","text":"<p>NextPlaid supports adding documents to existing indices:</p> <pre><code>client.add(\"my_index\", new_documents)\n</code></pre>"},{"location":"concepts/plaid/#update-process","title":"Update Process","text":"<ol> <li>Assign new tokens to existing centroids</li> <li>Compute and quantize residuals</li> <li>Update inverted file</li> <li>Optionally expand centroids if needed</li> </ol>"},{"location":"concepts/plaid/#centroid-expansion","title":"Centroid Expansion","text":"<p>When documents differ significantly from existing centroids:</p> <pre><code>UpdateConfig(\n    buffer_size=100,           # Docs before expansion check\n    max_points_per_centroid=256  # Expansion threshold\n)\n</code></pre>"},{"location":"concepts/plaid/#further-reading","title":"Further Reading","text":"<ul> <li>Multi-Vector Search - Why multi-vector search?</li> <li>PLAID Paper - Original research</li> <li>ColBERTv2 - Residual compression origins</li> </ul>"},{"location":"deployment/docker/","title":"Docker Deployment","text":"<p>Deploy NextPlaid using Docker containers.</p>"},{"location":"deployment/docker/#quick-start","title":"Quick Start","text":"<pre><code>docker pull ghcr.io/lightonai/next-plaid-api:latest\n\ndocker run -d \\\n  --name next-plaid-api \\\n  -p 8080:8080 \\\n  -v ~/.local/share/next-plaid:/data/indices \\\n  ghcr.io/lightonai/next-plaid-api:latest\n</code></pre> <p>Verify:</p> <pre><code>curl http://localhost:8080/health\n</code></pre>"},{"location":"deployment/docker/#image-variants","title":"Image Variants","text":"<p>The PLAID index and search always run on CPU. Model inference for text encoding can run on CPU or GPU depending on the image.</p> Tag Description Index &amp; Search Model Inference <code>latest</code> CPU image CPU CPU <code>X.Y.Z</code> Versioned CPU CPU CPU <code>latest-cuda</code> CUDA image CPU GPU <code>X.Y.Z-cuda</code> Versioned CUDA CPU GPU"},{"location":"deployment/docker/#docker-compose","title":"Docker Compose","text":""},{"location":"deployment/docker/#cpu-with-model-support-default","title":"CPU with Model Support (Default)","text":"<pre><code># docker-compose.yml\nservices:\n  next-plaid-api:\n    build:\n      context: .\n      dockerfile: next-plaid-api/Dockerfile\n      target: runtime-cpu\n    ports:\n      - \"8080:8080\"\n    volumes:\n      # Persistent index storage\n      - ${NEXT_PLAID_DATA:-~/.local/share/next-plaid}:/data/indices\n      # Persistent model cache (downloaded from HuggingFace)\n      - ${NEXT_PLAID_MODELS:-~/.cache/huggingface/next-plaid}:/models\n    environment:\n      - RUST_LOG=info\n    command:\n      [\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        \"8080\",\n        \"--index-dir\",\n        \"/data/indices\",\n        \"--model\",\n        \"lightonai/GTE-ModernColBERT-v1-onnx\",\n        \"--int8\",\n      ]\n    healthcheck:\n      test:\n        [\"CMD\", \"curl\", \"-f\", \"--max-time\", \"5\", \"http://localhost:8080/health\"]\n      interval: 15s\n      timeout: 5s\n      retries: 2\n      start_period: 120s\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          memory: 16G\n        reservations:\n          memory: 4G\n</code></pre>"},{"location":"deployment/docker/#cuda-gpu-encoding","title":"CUDA (GPU Encoding)","text":"<p>Use as an overlay with <code>docker compose -f docker-compose.yml -f docker-compose.cuda.yml up -d</code>:</p> <pre><code># docker-compose.cuda.yml\nservices:\n  next-plaid-api:\n    build:\n      target: runtime-cuda\n    environment:\n      - NVIDIA_VISIBLE_DEVICES=all\n    command:\n      [\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        \"8080\",\n        \"--index-dir\",\n        \"/data/indices\",\n        \"--model\",\n        \"lightonai/GTE-ModernColBERT-v1-onnx\",\n        \"--cuda\",\n      ]\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre>"},{"location":"deployment/docker/#running-docker-compose","title":"Running Docker Compose","text":"<pre><code># CPU with model support (default)\ndocker compose up -d\n\n# With CUDA (GPU encoding)\ndocker compose -f docker-compose.yml -f docker-compose.cuda.yml up -d\n\n# View logs\ndocker compose logs -f\n\n# Stop\ndocker compose down\n</code></pre>"},{"location":"deployment/docker/#configuration","title":"Configuration","text":""},{"location":"deployment/docker/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>RUST_LOG</code> <code>info</code> Log level <code>HF_TOKEN</code> - HuggingFace token for private models <code>NEXT_PLAID_DATA</code> <code>~/.local/share/next-plaid</code> Host path for persistent index storage <code>NEXT_PLAID_MODELS</code> <code>~/.cache/huggingface/next-plaid</code> Host path for downloaded model cache"},{"location":"deployment/docker/#volume-mounts","title":"Volume Mounts","text":"Container Path Default Host Path Purpose <code>/data/indices</code> <code>~/.local/share/next-plaid</code> Index storage (persistent) <code>/models</code> <code>~/.cache/huggingface/next-plaid</code> Model cache (auto-downloaded from HuggingFace) <p>Models are downloaded once from HuggingFace and cached locally. On subsequent container starts, cached models are reused without re-downloading.</p>"},{"location":"deployment/docker/#ports","title":"Ports","text":"Port Protocol Description 8080 HTTP API server"},{"location":"deployment/docker/#scaling","title":"Scaling","text":""},{"location":"deployment/docker/#single-instance","title":"Single Instance","text":"<p>For most use cases, a single container is sufficient:</p> <pre><code>deploy:\n  resources:\n    limits:\n      memory: 8G\n      cpus: \"4\"\n</code></pre>"},{"location":"deployment/docker/#multiple-instances-load-balancing","title":"Multiple Instances (Load Balancing)","text":"<p>For high throughput, run multiple instances behind a load balancer:</p> <pre><code>services:\n  next-plaid:\n    image: ghcr.io/lightonai/next-plaid-api:latest\n    deploy:\n      replicas: 3\n    volumes:\n      - /shared/indices:/data/indices:ro # Read-only shared storage\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - next-plaid\n</code></pre> <p>Replicas and Index Updates</p> <p>When running multiple replicas, do not update the index as NextPlaid does not currently handle replica synchronization. Each replica should either:</p> <pre><code>- Point to a **read-only** shared index (as shown above)\n- Point to a **distinct index** if write operations are required\n\nIndex updates on one replica will not propagate to others and may cause data inconsistencies.\n</code></pre>"},{"location":"deployment/docker/#health-checks","title":"Health Checks","text":"<p>The container includes a health check:</p> <pre><code>HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:8080/health || exit 1\n</code></pre>"},{"location":"deployment/docker/#kubernetes-probes","title":"Kubernetes Probes","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 30\n\nreadinessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\n</code></pre>"},{"location":"deployment/docker/#logging","title":"Logging","text":""},{"location":"deployment/docker/#log-levels","title":"Log Levels","text":"<p>Set via <code>RUST_LOG</code> environment variable:</p> Level Description <code>error</code> Errors only <code>warn</code> Warnings and errors <code>info</code> General information (default) <code>debug</code> Detailed debugging <code>trace</code> Very verbose"},{"location":"deployment/docker/#json-logging","title":"JSON Logging","text":"<p>For production, configure structured logging:</p> <pre><code>docker run -e RUST_LOG=info,tower_http=debug \\\n  ghcr.io/lightonai/next-plaid-api:latest\n</code></pre>"},{"location":"python-sdk/","title":"Python SDK","text":"<p>The official Python client for the NextPlaid API.</p>"},{"location":"python-sdk/#installation","title":"Installation","text":"<pre><code>pip install next-plaid-client\n</code></pre>"},{"location":"python-sdk/#quick-start","title":"Quick Start","text":"<pre><code>from next_plaid_client import NextPlaidClient, IndexConfig, SearchParams\n\n# Connect to the API\nclient = NextPlaidClient(\"http://localhost:8080\")\n\n# Create an index\nclient.create_index(\"my_index\", IndexConfig(nbits=4))\n\n# Add documents with embeddings\ndocuments = [{\"embeddings\": [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]}]\nmetadata = [{\"title\": \"Document 1\"}]\nclient.add(\"my_index\", documents, metadata)\n\n# Search\nresults = client.search(\n    \"my_index\",\n    queries=[[[0.1, 0.2, 0.3]]],\n    params=SearchParams(top_k=10)\n)\n\nfor result in results.results:\n    print(f\"Found {len(result.document_ids)} documents\")\n</code></pre>"},{"location":"python-sdk/#clients","title":"Clients","text":"<p>NextPlaid provides two client implementations:</p> Client Use Case <code>NextPlaidClient</code> Synchronous operations, simple scripts <code>AsyncNextPlaidClient</code> Async/await, high-throughput applications <p>Both clients have identical APIs, differing only in sync vs async methods.</p>"},{"location":"python-sdk/#basic-usage-patterns","title":"Basic Usage Patterns","text":""},{"location":"python-sdk/#context-manager","title":"Context Manager","text":"<pre><code># Synchronous\nwith NextPlaidClient(\"http://localhost:8080\") as client:\n    indices = client.list_indices()\n\n# Asynchronous\nasync with AsyncNextPlaidClient(\"http://localhost:8080\") as client:\n    indices = await client.list_indices()\n</code></pre>"},{"location":"python-sdk/#error-handling","title":"Error Handling","text":"<pre><code>from next_plaid_client import (\n    NextPlaidClient,\n    IndexNotFoundError,\n    IndexExistsError,\n    ValidationError,\n    RateLimitError,\n)\n\nclient = NextPlaidClient(\"http://localhost:8080\")\n\ntry:\n    client.create_index(\"my_index\")\nexcept IndexExistsError:\n    print(\"Index already exists\")\n\ntry:\n    client.get_index(\"nonexistent\")\nexcept IndexNotFoundError:\n    print(\"Index not found\")\n</code></pre>"},{"location":"python-sdk/#working-with-embeddings","title":"Working with Embeddings","text":"<p>Documents and queries are represented as multi-dimensional arrays:</p> <pre><code># Document: list of token embeddings [num_tokens, embedding_dim]\ndocument = {\n    \"embeddings\": [\n        [0.1, 0.2, 0.3, ...],  # Token 1\n        [0.4, 0.5, 0.6, ...],  # Token 2\n        [0.7, 0.8, 0.9, ...],  # Token 3\n    ]\n}\n\n# Query: same structure\nquery = [[0.1, 0.2, 0.3, ...], [0.4, 0.5, 0.6, ...]]\n\n# Search expects a list of queries\nresults = client.search(\"my_index\", queries=[query])\n</code></pre>"},{"location":"python-sdk/#metadata-filtering","title":"Metadata Filtering","text":"<pre><code># Add documents with metadata\nmetadata = [\n    {\"title\": \"Doc 1\", \"category\": \"science\", \"year\": 2023},\n    {\"title\": \"Doc 2\", \"category\": \"history\", \"year\": 2022},\n]\nclient.add(\"my_index\", documents, metadata)\n\n# Search with SQL-like filter\nresults = client.search(\n    \"my_index\",\n    queries=[query],\n    filter_condition=\"category = ? AND year &gt;= ?\",\n    filter_parameters=[\"science\", 2020],\n)\n</code></pre>"},{"location":"python-sdk/#text-encoding-requires-model","title":"Text Encoding (Requires Model)","text":"<p>If the server is running with a model loaded, the <code>add</code> and <code>search</code> methods automatically detect text input:</p> <pre><code># Add documents from text (auto-detected)\nclient.add(\n    \"my_index\",\n    [\"Paris is the capital of France.\"],\n    metadata=[{\"country\": \"France\"}]\n)\n\n# Search with text queries (auto-detected)\nresults = client.search(\n    \"my_index\",\n    queries=[\"What is the capital of France?\"],\n)\n</code></pre>"},{"location":"python-sdk/#configuration","title":"Configuration","text":""},{"location":"python-sdk/#client-options","title":"Client Options","text":"<pre><code>client = NextPlaidClient(\n    base_url=\"http://localhost:8080\",  # API server URL\n    timeout=30.0,                       # Request timeout in seconds\n    headers={\"Authorization\": \"...\"},   # Custom headers\n)\n</code></pre>"},{"location":"python-sdk/#search-parameters","title":"Search Parameters","text":"<pre><code>from next_plaid_client import SearchParams\n\nparams = SearchParams(\n    top_k=10,           # Number of results to return\n    n_ivf_probe=8,      # Number of IVF cells to probe\n    n_full_scores=4096, # Candidates for full scoring\n)\n</code></pre>"},{"location":"python-sdk/#api-reference","title":"API Reference","text":"<ul> <li>Sync Client - <code>NextPlaidClient</code> reference</li> <li>Async Client - <code>AsyncNextPlaidClient</code> reference</li> <li>Models - Data models and types</li> </ul>"},{"location":"python-sdk/async-client/","title":"Async Client","text":"<p><code>AsyncNextPlaidClient</code> provides asynchronous access to the NextPlaid API using <code>async</code>/<code>await</code>.</p>"},{"location":"python-sdk/async-client/#initialization","title":"Initialization","text":"<pre><code>from next_plaid_client import AsyncNextPlaidClient\n\nclient = AsyncNextPlaidClient(\n    base_url=\"http://localhost:8080\",\n    timeout=30.0,\n    headers=None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>base_url</code> <code>str</code> <code>\"http://localhost:8080\"</code> API server URL <code>timeout</code> <code>float</code> <code>30.0</code> Request timeout in seconds <code>headers</code> <code>Dict[str, str]</code> <code>None</code> Custom headers for all requests"},{"location":"python-sdk/async-client/#async-context-manager","title":"Async Context Manager","text":"<pre><code>async with AsyncNextPlaidClient(\"http://localhost:8080\") as client:\n    health = await client.health()\n    # Client is automatically closed when exiting the block\n</code></pre>"},{"location":"python-sdk/async-client/#usage-example","title":"Usage Example","text":"<pre><code>import asyncio\nfrom next_plaid_client import AsyncNextPlaidClient, IndexConfig, SearchParams\n\nasync def main():\n    async with AsyncNextPlaidClient(\"http://localhost:8080\") as client:\n        # Check health\n        health = await client.health()\n        print(f\"Status: {health.status}\")\n\n        # Create index\n        await client.create_index(\"async_index\", IndexConfig(nbits=4))\n\n        # Add documents (text - requires model on server)\n        await client.add(\n            \"async_index\",\n            [\"Paris is the capital of France.\"],\n            metadata=[{\"country\": \"France\"}]\n        )\n\n        # Or add with embeddings\n        await client.add(\"async_index\", [{\"embeddings\": [[0.1, 0.2], [0.3, 0.4]]}])\n\n        # Search with text\n        results = await client.search(\n            \"async_index\",\n            [\"What is the capital of France?\"],\n            params=SearchParams(top_k=5)\n        )\n        print(f\"Found: {results.num_queries} queries processed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"python-sdk/async-client/#concurrent-operations","title":"Concurrent Operations","text":"<p>The async client can perform concurrent operations:</p> <pre><code>async def search_multiple_indices(client, queries):\n    \"\"\"Search multiple indices concurrently.\"\"\"\n    tasks = [\n        client.search(f\"index_{i}\", queries)\n        for i in range(10)\n    ]\n    results = await asyncio.gather(*tasks)\n    return results\n\nasync def batch_add_documents(client, index_name, document_batches):\n    \"\"\"Add multiple document batches concurrently.\"\"\"\n    tasks = [\n        client.add(index_name, batch)\n        for batch in document_batches\n    ]\n    await asyncio.gather(*tasks)\n</code></pre>"},{"location":"python-sdk/async-client/#api-reference","title":"API Reference","text":"<p>The async client has the same methods as the sync client, but all methods are <code>async</code>:</p>"},{"location":"python-sdk/async-client/#health-monitoring","title":"Health &amp; Monitoring","text":"<pre><code>health = await client.health()\n</code></pre>"},{"location":"python-sdk/async-client/#index-management","title":"Index Management","text":"<pre><code>indices = await client.list_indices()\ninfo = await client.get_index(\"my_index\")\nawait client.create_index(\"my_index\", IndexConfig())\nawait client.delete_index(\"my_index\")\nawait client.update_index_config(\"my_index\", max_documents=1000)\n</code></pre>"},{"location":"python-sdk/async-client/#document-management","title":"Document Management","text":"<pre><code># Add documents (auto-detects text vs embeddings)\nawait client.add(\"my_index\", [\"Document text\"])  # text (requires model)\nawait client.add(\"my_index\", [{\"embeddings\": [[0.1, 0.2]]}])  # embeddings\n\n# Delete documents by metadata filter (async, returns 202 Accepted)\nresult = await client.delete(\n    \"my_index\",\n    condition=\"category = ? AND year &lt; ?\",\n    parameters=[\"outdated\", 2020]\n)\n</code></pre>"},{"location":"python-sdk/async-client/#search-operations","title":"Search Operations","text":"<pre><code># Search (auto-detects text vs embeddings)\nresults = await client.search(\"my_index\", [\"query text\"])  # text\nresults = await client.search(\"my_index\", [[[0.1, 0.2]]])  # embeddings\n\n# Search with filter\nresults = await client.search(\n    \"my_index\",\n    [\"machine learning\"],\n    filter_condition=\"category = ?\",\n    filter_parameters=[\"science\"]\n)\n\n# Search with parameters\nresults = await client.search(\n    \"my_index\",\n    [\"query\"],\n    params=SearchParams(top_k=10, n_ivf_probe=16)\n)\n</code></pre>"},{"location":"python-sdk/async-client/#metadata-management","title":"Metadata Management","text":"<pre><code>response = await client.get_metadata(\"my_index\")\nawait client.add_metadata(\"my_index\", metadata)\nresult = await client.query_metadata(\"my_index\", condition, parameters)\nresponse = await client.get_metadata_by_ids(\"my_index\", document_ids)\ncount = await client.get_metadata_count(\"my_index\")\ncheck = await client.check_metadata(\"my_index\", document_ids)\n</code></pre>"},{"location":"python-sdk/async-client/#text-encoding","title":"Text Encoding","text":"<pre><code>response = await client.encode(texts, input_type=\"document\")\n</code></pre>"},{"location":"python-sdk/async-client/#cleanup","title":"Cleanup","text":"<pre><code>await client.close()\n</code></pre>"},{"location":"python-sdk/async-client/#integration-with-web-frameworks","title":"Integration with Web Frameworks","text":""},{"location":"python-sdk/async-client/#fastapi","title":"FastAPI","text":"<pre><code>from fastapi import FastAPI, Depends\nfrom next_plaid_client import AsyncNextPlaidClient, SearchParams\n\napp = FastAPI()\nclient = AsyncNextPlaidClient(\"http://localhost:8080\")\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    await client.close()\n\n@app.post(\"/search\")\nasync def search(query: str):\n    # Search with text (requires model on server)\n    results = await client.search(\n        \"my_index\",\n        [query],\n        params=SearchParams(top_k=10)\n    )\n    return results.results[0].document_ids\n</code></pre>"},{"location":"python-sdk/async-client/#aiohttp","title":"aiohttp","text":"<pre><code>from aiohttp import web\nfrom next_plaid_client import AsyncNextPlaidClient\n\nasync def init_app():\n    app = web.Application()\n    app[\"plaid_client\"] = AsyncNextPlaidClient(\"http://localhost:8080\")\n\n    async def cleanup(app):\n        await app[\"plaid_client\"].close()\n\n    app.on_cleanup.append(cleanup)\n    return app\n\nasync def search_handler(request):\n    client = request.app[\"plaid_client\"]\n    results = await client.search(\"my_index\", [\"query text\"])\n    return web.json_response({\"results\": results.results})\n</code></pre>"},{"location":"python-sdk/async-client/#error-handling","title":"Error Handling","text":"<pre><code>from next_plaid_client import (\n    AsyncNextPlaidClient,\n    IndexNotFoundError,\n    RateLimitError,\n)\n\nasync def safe_search(client, index_name, queries):\n    try:\n        return await client.search(index_name, queries)\n    except IndexNotFoundError:\n        print(f\"Index {index_name} not found\")\n        return None\n    except RateLimitError:\n        print(\"Rate limited, retrying...\")\n        await asyncio.sleep(1)\n        return await safe_search(client, index_name, queries)\n</code></pre>"},{"location":"python-sdk/async-client/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom next_plaid_client import (\n    AsyncNextPlaidClient,\n    IndexConfig,\n    SearchParams,\n)\n\nasync def main():\n    async with AsyncNextPlaidClient(\"http://localhost:8080\") as client:\n        # Setup\n        indices = await client.list_indices()\n        if \"demo\" not in indices:\n            await client.create_index(\"demo\", IndexConfig(nbits=4))\n\n        # Add documents with text (requires model on server)\n        await client.add(\n            \"demo\",\n            [\n                \"The Eiffel Tower is in Paris, France.\",\n                \"The Great Wall is in China.\",\n                \"The Colosseum is in Rome, Italy.\",\n            ],\n            metadata=[\n                {\"landmark\": \"Eiffel Tower\", \"country\": \"France\"},\n                {\"landmark\": \"Great Wall\", \"country\": \"China\"},\n                {\"landmark\": \"Colosseum\", \"country\": \"Italy\"},\n            ]\n        )\n\n        # Search with text\n        results = await client.search(\n            \"demo\",\n            [\"Famous landmarks in Europe\"],\n            params=SearchParams(top_k=3)\n        )\n\n        # Get metadata for results\n        doc_ids = results.results[0].document_ids\n        metadata = await client.get_metadata_by_ids(\"demo\", doc_ids)\n\n        for doc_id, score, meta in zip(\n            results.results[0].document_ids,\n            results.results[0].scores,\n            metadata.metadata\n        ):\n            print(f\"{meta['landmark']}: {score:.4f}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"python-sdk/client/","title":"Sync Client","text":"<p><code>NextPlaidClient</code> provides synchronous access to the NextPlaid API.</p>"},{"location":"python-sdk/client/#initialization","title":"Initialization","text":"<pre><code>from next_plaid_client import NextPlaidClient\n\nclient = NextPlaidClient(\n    base_url=\"http://localhost:8080\",\n    timeout=30.0,\n    headers=None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>base_url</code> <code>str</code> <code>\"http://localhost:8080\"</code> API server URL <code>timeout</code> <code>float</code> <code>30.0</code> Request timeout in seconds <code>headers</code> <code>Dict[str, str]</code> <code>None</code> Custom headers for all requests"},{"location":"python-sdk/client/#context-manager","title":"Context Manager","text":"<pre><code>with NextPlaidClient(\"http://localhost:8080\") as client:\n    health = client.health()\n    # Client is automatically closed when exiting the block\n</code></pre>"},{"location":"python-sdk/client/#health-monitoring","title":"Health &amp; Monitoring","text":""},{"location":"python-sdk/client/#health","title":"<code>health()</code>","text":"<p>Check server health and status.</p> <pre><code>health = client.health()\nprint(health.status)           # \"healthy\"\nprint(health.loaded_indices)   # 3\nprint(health.memory_usage_bytes)\n</code></pre> <p>Returns: <code>HealthResponse</code></p>"},{"location":"python-sdk/client/#index-management","title":"Index Management","text":""},{"location":"python-sdk/client/#list_indices","title":"<code>list_indices()</code>","text":"<p>List all available indices.</p> <pre><code>indices = client.list_indices()\n# ['index1', 'index2', 'index3']\n</code></pre> <p>Returns: <code>List[str]</code></p>"},{"location":"python-sdk/client/#get_indexname","title":"<code>get_index(name)</code>","text":"<p>Get detailed information about an index.</p> <pre><code>info = client.get_index(\"my_index\")\nprint(info.num_documents)    # 1000\nprint(info.num_embeddings)   # 15000\nprint(info.dimension)        # 128\nprint(info.has_metadata)     # True\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>name</code> <code>str</code> Index name <p>Returns: <code>IndexInfo</code></p> <p>Raises: <code>IndexNotFoundError</code></p>"},{"location":"python-sdk/client/#create_indexname-confignone","title":"<code>create_index(name, config=None)</code>","text":"<p>Create a new index.</p> <pre><code>from next_plaid_client import IndexConfig\n\nclient.create_index(\"my_index\", IndexConfig(nbits=4))\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>name</code> <code>str</code> Index name <code>config</code> <code>IndexConfig</code> Configuration (optional) <p>Returns: <code>Dict[str, Any]</code></p> <p>Raises: <code>IndexExistsError</code>, <code>ValidationError</code></p>"},{"location":"python-sdk/client/#delete_indexname","title":"<code>delete_index(name)</code>","text":"<p>Delete an index and all its data.</p> <pre><code>client.delete_index(\"my_index\")\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>name</code> <code>str</code> Index name <p>Returns: <code>Dict[str, Any]</code></p> <p>Raises: <code>IndexNotFoundError</code></p>"},{"location":"python-sdk/client/#update_index_configname-max_documents","title":"<code>update_index_config(name, max_documents)</code>","text":"<p>Update index configuration.</p> <pre><code>client.update_index_config(\"my_index\", max_documents=10000)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>name</code> <code>str</code> Index name <code>max_documents</code> <code>int | None</code> Max documents (None to remove limit) <p>Returns: <code>Dict[str, Any]</code></p>"},{"location":"python-sdk/client/#document-management","title":"Document Management","text":""},{"location":"python-sdk/client/#addindex_name-documents-metadatanone","title":"<code>add(index_name, documents, metadata=None)</code>","text":"<p>Add documents to an index. Automatically detects input type.</p> <p>This method accepts either:</p> <ul> <li>Text documents (<code>List[str]</code>): Server encodes them (requires model)</li> <li>Embeddings (<code>List[Dict]</code> or <code>List[Document]</code>): Pre-computed embeddings</li> </ul> <pre><code># Add text documents (requires model on server)\nclient.add(\"my_index\", [\"Document 1 text\", \"Document 2 text\"])\n\n# Add documents with pre-computed embeddings\nclient.add(\"my_index\", [{\"embeddings\": [[0.1, 0.2], [0.3, 0.4]]}])\n\n# Add with metadata\nclient.add(\n    \"my_index\",\n    [\"Paris is in France\", \"Berlin is in Germany\"],\n    metadata=[{\"country\": \"France\"}, {\"country\": \"Germany\"}]\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>index_name</code> <code>str</code> Index name <code>documents</code> <code>List[str]</code> or <code>List[Dict]</code> Text strings or embedding dicts <code>metadata</code> <code>List[Dict]</code> Metadata for each document (optional) <p>Returns: <code>str</code> (status message)</p> <p>Raises: <code>IndexNotFoundError</code>, <code>ModelNotLoadedError</code> (for text input), <code>ValidationError</code></p>"},{"location":"python-sdk/client/#deleteindex_name-condition-parametersnone","title":"<code>delete(index_name, condition, parameters=None)</code>","text":"<p>Delete documents by metadata filter. The operation is asynchronous and returns immediately.</p> <pre><code># Delete documents matching a condition\nresult = client.delete(\n    \"my_index\",\n    condition=\"category = ? AND year &lt; ?\",\n    parameters=[\"outdated\", 2020]\n)\nprint(result)  # \"Delete queued: 15 documents matching condition\"\n\n# Delete all documents in a category\nresult = client.delete(\n    \"my_index\",\n    condition=\"category = ?\",\n    parameters=[\"archived\"]\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>index_name</code> <code>str</code> Index name <code>condition</code> <code>str</code> SQL WHERE condition for selecting documents to delete <code>parameters</code> <code>List[Any]</code> Parameters for the condition (optional) <p>Returns: <code>str</code> (status message indicating queued deletion count)</p> <p>Raises: <code>IndexNotFoundError</code>, <code>MetadataNotFoundError</code>, <code>ValidationError</code></p>"},{"location":"python-sdk/client/#search-operations","title":"Search Operations","text":""},{"location":"python-sdk/client/#searchindex_name-queries-paramsnone-filter_conditionnone-filter_parametersnone-subsetnone","title":"<code>search(index_name, queries, params=None, filter_condition=None, filter_parameters=None, subset=None)</code>","text":"<p>Search an index. Automatically detects query input type.</p> <p>This method accepts either:</p> <ul> <li>Text queries (<code>List[str]</code>): Server encodes them (requires model)</li> <li>Embedding queries (<code>List[List[List[float]]]</code>): Pre-computed embeddings</li> </ul> <pre><code>from next_plaid_client import SearchParams\n\n# Search with text queries (requires model on server)\nresults = client.search(\"my_index\", [\"What is AI?\"])\n\n# Search with pre-computed embeddings\nresults = client.search(\"my_index\", [[[0.1, 0.2], [0.3, 0.4]]])\n\n# Search with metadata filter\nresults = client.search(\n    \"my_index\",\n    [\"machine learning\"],\n    filter_condition=\"category = ? AND year &gt;= ?\",\n    filter_parameters=[\"science\", 2020]\n)\n\n# Search with parameters\nresults = client.search(\n    \"my_index\",\n    [\"query text\"],\n    params=SearchParams(top_k=5, n_ivf_probe=16)\n)\n\n# Search within a subset of documents\nresults = client.search(\"my_index\", [\"query\"], subset=[0, 5, 10])\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>index_name</code> <code>str</code> Index name <code>queries</code> <code>List[str]</code> or <code>List[List[List[float]]]</code> Text or embedding queries <code>params</code> <code>SearchParams</code> Search parameters (optional) <code>filter_condition</code> <code>str</code> SQL WHERE condition for filtering (optional) <code>filter_parameters</code> <code>List[Any]</code> Parameters for filter condition (optional) <code>subset</code> <code>List[int]</code> Limit search to these doc IDs (optional) <p>Returns: <code>SearchResult</code></p> <p>Raises: <code>IndexNotFoundError</code>, <code>ModelNotLoadedError</code> (for text queries)</p>"},{"location":"python-sdk/client/#metadata-management","title":"Metadata Management","text":""},{"location":"python-sdk/client/#get_metadataindex_name","title":"<code>get_metadata(index_name)</code>","text":"<p>Get all metadata entries.</p> <pre><code>response = client.get_metadata(\"my_index\")\nfor entry in response.metadata:\n    print(entry)\n</code></pre> <p>Returns: <code>MetadataResponse</code></p>"},{"location":"python-sdk/client/#add_metadataindex_name-metadata","title":"<code>add_metadata(index_name, metadata)</code>","text":"<p>Add or update metadata entries.</p> <pre><code>metadata = [\n    {\"_id\": 0, \"title\": \"Updated Title\"},\n    {\"_id\": 1, \"category\": \"new_category\"},\n]\nclient.add_metadata(\"my_index\", metadata)\n</code></pre>"},{"location":"python-sdk/client/#query_metadataindex_name-condition-parametersnone","title":"<code>query_metadata(index_name, condition, parameters=None)</code>","text":"<p>Query metadata using SQL conditions.</p> <pre><code>result = client.query_metadata(\n    \"my_index\",\n    condition=\"category = ? AND score &gt; ?\",\n    parameters=[\"science\", 0.8],\n)\nprint(result[\"document_ids\"])  # [0, 3, 7]\nprint(result[\"count\"])         # 3\n</code></pre>"},{"location":"python-sdk/client/#get_metadata_by_idsindex_name-document_idsnone-conditionnone-parametersnone-limitnone","title":"<code>get_metadata_by_ids(index_name, document_ids=None, condition=None, parameters=None, limit=None)</code>","text":"<p>Get metadata by document IDs or condition.</p> <pre><code>response = client.get_metadata_by_ids(\n    \"my_index\",\n    document_ids=[0, 1, 2],\n)\n</code></pre>"},{"location":"python-sdk/client/#text-encoding","title":"Text Encoding","text":""},{"location":"python-sdk/client/#encodetexts-input_typedocument","title":"<code>encode(texts, input_type=\"document\")</code>","text":"<p>Encode texts into ColBERT embeddings (requires model).</p> <pre><code>response = client.encode(\n    texts=[\"Hello world\", \"Machine learning\"],\n    input_type=\"document\",  # or \"query\"\n)\nprint(response.num_texts)  # 2\nprint(len(response.embeddings[0]))  # num_tokens\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>texts</code> <code>List[str]</code> Texts to encode <code>input_type</code> <code>str</code> <code>\"document\"</code> or <code>\"query\"</code> <p>Returns: <code>EncodeResponse</code></p> <p>Raises: <code>ModelNotLoadedError</code></p>"},{"location":"python-sdk/client/#cleanup","title":"Cleanup","text":""},{"location":"python-sdk/client/#close","title":"<code>close()</code>","text":"<p>Close the HTTP client.</p> <pre><code>client.close()\n</code></pre> <p>Or use the context manager for automatic cleanup.</p>"},{"location":"python-sdk/models/","title":"Data Models","text":"<p>Reference for all data models used in the NextPlaid Python SDK.</p>"},{"location":"python-sdk/models/#configuration-models","title":"Configuration Models","text":""},{"location":"python-sdk/models/#indexconfig","title":"IndexConfig","text":"<p>Configuration for creating a new index.</p> <pre><code>from next_plaid_client import IndexConfig\n\nconfig = IndexConfig(\n    nbits=4,\n    batch_size=50000,\n    seed=42,\n    start_from_scratch=999,\n    max_documents=100000,\n)\n</code></pre> Field Type Default Description <code>nbits</code> <code>int</code> <code>4</code> Quantization bits (2 or 4). Lower = faster, less accurate <code>batch_size</code> <code>int</code> <code>50000</code> Tokens per batch during indexing <code>seed</code> <code>int \\| None</code> <code>None</code> Random seed for reproducibility <code>start_from_scratch</code> <code>int</code> <code>999</code> Rebuild threshold for small indices <code>max_documents</code> <code>int \\| None</code> <code>None</code> Maximum documents allowed"},{"location":"python-sdk/models/#searchparams","title":"SearchParams","text":"<p>Parameters for search operations.</p> <pre><code>from next_plaid_client import SearchParams\n\nparams = SearchParams(\n    top_k=10,\n    n_ivf_probe=8,\n    n_full_scores=4096,\n    centroid_score_threshold=0.4,\n)\n</code></pre> Field Type Default Description <code>top_k</code> <code>int</code> <code>10</code> Number of results to return <code>n_ivf_probe</code> <code>int</code> <code>8</code> Number of IVF cells to probe <code>n_full_scores</code> <code>int</code> <code>4096</code> Candidates for exact scoring <code>centroid_score_threshold</code> <code>float \\| None</code> <code>0.4</code> Centroid pruning threshold. Set to <code>None</code> to disable <p>Tuning Guidelines:</p> <ul> <li>Increase <code>n_ivf_probe</code> for better recall (slower)</li> <li>Increase <code>n_full_scores</code> for more accurate ranking (slower)</li> <li>Decrease both for faster but less accurate search</li> <li>Increase <code>centroid_score_threshold</code> (0.45-0.5) for faster search with smaller k</li> <li>Decrease <code>centroid_score_threshold</code> (0.3-0.4) for better recall with larger k</li> <li>Set <code>centroid_score_threshold=None</code> to disable pruning (slowest but most accurate)</li> </ul>"},{"location":"python-sdk/models/#response-models","title":"Response Models","text":""},{"location":"python-sdk/models/#healthresponse","title":"HealthResponse","text":"<p>Response from the health endpoint.</p> <pre><code>health = client.health()\nprint(health.status)              # \"healthy\"\nprint(health.version)             # \"0.1.0\"\nprint(health.loaded_indices)      # 3\nprint(health.index_dir)           # \"/data/indices\"\nprint(health.memory_usage_bytes)  # 1234567890\nprint(health.indices)             # List[IndexSummary]\n</code></pre> Field Type Description <code>status</code> <code>str</code> Server status (\"healthy\") <code>version</code> <code>str</code> API version <code>loaded_indices</code> <code>int</code> Number of loaded indices <code>index_dir</code> <code>str</code> Index storage directory <code>memory_usage_bytes</code> <code>int</code> Memory usage <code>indices</code> <code>List[IndexSummary]</code> Summary of all indices"},{"location":"python-sdk/models/#indexinfo","title":"IndexInfo","text":"<p>Detailed information about an index.</p> <pre><code>info = client.get_index(\"my_index\")\nprint(info.name)            # \"my_index\"\nprint(info.num_documents)   # 1000\nprint(info.num_embeddings)  # 15000\nprint(info.num_partitions)  # 64\nprint(info.avg_doclen)      # 15.0\nprint(info.dimension)       # 128\nprint(info.has_metadata)    # True\nprint(info.metadata_count)  # 1000\nprint(info.max_documents)   # 100000\n</code></pre> Field Type Description <code>name</code> <code>str</code> Index name <code>num_documents</code> <code>int</code> Number of documents <code>num_embeddings</code> <code>int</code> Total token embeddings <code>num_partitions</code> <code>int</code> Number of IVF partitions <code>avg_doclen</code> <code>float</code> Average tokens per document <code>dimension</code> <code>int</code> Embedding dimension <code>has_metadata</code> <code>bool</code> Whether metadata exists <code>metadata_count</code> <code>int \\| None</code> Number of metadata entries <code>max_documents</code> <code>int \\| None</code> Maximum documents limit"},{"location":"python-sdk/models/#indexsummary","title":"IndexSummary","text":"<p>Summary information (from health endpoint).</p> Field Type Description <code>name</code> <code>str</code> Index name <code>num_documents</code> <code>int</code> Number of documents <code>num_embeddings</code> <code>int</code> Total embeddings <code>num_partitions</code> <code>int</code> IVF partitions <code>dimension</code> <code>int</code> Embedding dimension <code>nbits</code> <code>int</code> Quantization bits <code>avg_doclen</code> <code>float</code> Average document length <code>has_metadata</code> <code>bool</code> Has metadata <code>max_documents</code> <code>int \\| None</code> Max documents"},{"location":"python-sdk/models/#searchresult","title":"SearchResult","text":"<p>Response from search operations.</p> <pre><code>results = client.search(\"my_index\", queries=[...])\nprint(results.num_queries)  # 1\nfor result in results.results:\n    print(result.query_id)\n    print(result.document_ids)  # [0, 5, 3, ...]\n    print(result.scores)        # [0.95, 0.87, 0.82, ...]\n    print(result.metadata)      # [{\"title\": ...}, ...]\n</code></pre> Field Type Description <code>results</code> <code>List[QueryResult]</code> Results for each query <code>num_queries</code> <code>int</code> Number of queries processed"},{"location":"python-sdk/models/#queryresult","title":"QueryResult","text":"<p>Result for a single query.</p> Field Type Description <code>query_id</code> <code>int</code> Query index <code>document_ids</code> <code>List[int]</code> Matching document IDs <code>scores</code> <code>List[float]</code> Relevance scores <code>metadata</code> <code>List[Dict] \\| None</code> Document metadata"},{"location":"python-sdk/models/#metadataresponse","title":"MetadataResponse","text":"<p>Response from metadata operations.</p> <pre><code>response = client.get_metadata(\"my_index\")\nprint(response.count)     # 1000\nprint(response.metadata)  # [{\"_id\": 0, \"title\": ...}, ...]\n</code></pre> Field Type Description <code>metadata</code> <code>List[Dict[str, Any]]</code> Metadata entries <code>count</code> <code>int</code> Number of entries"},{"location":"python-sdk/models/#metadatacheckresponse","title":"MetadataCheckResponse","text":"<p>Response from metadata check operation.</p> <pre><code>check = client.check_metadata(\"my_index\", [0, 1, 2, 999])\nprint(check.existing_ids)    # [0, 1, 2]\nprint(check.missing_ids)     # [999]\nprint(check.existing_count)  # 3\nprint(check.missing_count)   # 1\n</code></pre> Field Type Description <code>existing_ids</code> <code>List[int]</code> IDs that exist <code>missing_ids</code> <code>List[int]</code> IDs that don't exist <code>existing_count</code> <code>int</code> Count of existing <code>missing_count</code> <code>int</code> Count of missing"},{"location":"python-sdk/models/#encoderesponse","title":"EncodeResponse","text":"<p>Response from text encoding.</p> <pre><code>response = client.encode([\"Hello world\"])\nprint(response.num_texts)    # 1\nprint(response.embeddings)   # [[[0.1, 0.2, ...], [0.3, 0.4, ...]]]\n</code></pre> Field Type Description <code>embeddings</code> <code>List[List[List[float]]]</code> Token embeddings <code>num_texts</code> <code>int</code> Number of texts encoded"},{"location":"python-sdk/models/#delete-documents-response","title":"Delete Documents Response","text":"<p>The delete operation is asynchronous and returns a string message indicating how many documents were queued for deletion.</p> <pre><code>result = client.delete(\n    \"my_index\",\n    condition=\"category = ? AND year &lt; ?\",\n    parameters=[\"outdated\", 2020]\n)\nprint(result)  # \"Delete queued: 15 documents matching condition\"\n</code></pre> <p>Asynchronous Operation</p> <p>The delete endpoint returns HTTP 202 Accepted immediately. The actual deletion happens in the background. Use the index info endpoint to verify document counts after deletion completes.</p>"},{"location":"python-sdk/models/#input-models","title":"Input Models","text":""},{"location":"python-sdk/models/#document","title":"Document","text":"<p>A document with embeddings for indexing.</p> <pre><code>from next_plaid_client import Document\n\ndoc = Document(embeddings=[[0.1, 0.2], [0.3, 0.4]])\n</code></pre> Field Type Description <code>embeddings</code> <code>List[List[float]]</code> Token embeddings [num_tokens, dim] <p>You can also use plain dictionaries:</p> <pre><code>doc = {\"embeddings\": [[0.1, 0.2], [0.3, 0.4]]}\n</code></pre>"},{"location":"python-sdk/models/#exceptions","title":"Exceptions","text":""},{"location":"python-sdk/models/#nextplaiderror","title":"NextPlaidError","text":"<p>Base exception for all API errors.</p> <pre><code>from next_plaid_client import NextPlaidError\n\ntry:\n    client.get_index(\"missing\")\nexcept NextPlaidError as e:\n    print(e.message)\n    print(e.code)\n</code></pre>"},{"location":"python-sdk/models/#indexnotfounderror","title":"IndexNotFoundError","text":"<p>Raised when an index doesn't exist.</p> <pre><code>from next_plaid_client import IndexNotFoundError\n</code></pre>"},{"location":"python-sdk/models/#indexexistserror","title":"IndexExistsError","text":"<p>Raised when creating an index that already exists.</p> <pre><code>from next_plaid_client import IndexExistsError\n</code></pre>"},{"location":"python-sdk/models/#validationerror","title":"ValidationError","text":"<p>Raised for invalid input data.</p> <pre><code>from next_plaid_client import ValidationError\n</code></pre>"},{"location":"python-sdk/models/#ratelimiterror","title":"RateLimitError","text":"<p>Raised when rate limited (429 status).</p> <pre><code>from next_plaid_client import RateLimitError\n</code></pre>"},{"location":"python-sdk/models/#modelnotloadederror","title":"ModelNotLoadedError","text":"<p>Raised when text encoding is requested but no model is loaded.</p> <pre><code>from next_plaid_client import ModelNotLoadedError\n</code></pre>"}]}