# =============================================================================
# Docker Compose override for CUDA support
# Usage: docker compose -f docker-compose.yml -f docker-compose.cuda.yml up -d
# Or: make docker-up-cuda
#
# Vector Database Storage:
#   Indices are persisted at ${NEXT_PLAID_DATA:-~/.local/share/next-plaid}
#
# Model Cache:
#   Downloaded HuggingFace models are cached at ${NEXT_PLAID_MODELS:-~/.cache/huggingface/next-plaid}
#
# CUDA Defaults (optimized for GPU):
#   --model lightonai/GTE-ModernColBERT-v1 --cuda --batch-size 64
#   (no --int8: GPU is fast enough with FP32, no --parallel: GPU handles parallelism)
#
# Rate Limiting & Concurrency (via environment variables):
#   RATE_LIMIT_PER_SECOND       Max requests per second (default: 50)
#   RATE_LIMIT_BURST_SIZE       Burst size for rate limiting (default: 100)
#   CONCURRENCY_LIMIT           Max concurrent in-flight requests (default: 100)
#   MAX_QUEUED_TASKS_PER_INDEX  Max queued updates/deletes per index (default: 10)
#   MAX_BATCH_DOCUMENTS         Max documents to batch before processing (default: 300)
#   BATCH_CHANNEL_SIZE          Buffer size for document batch queue (default: 100)
#   MAX_BATCH_TEXTS             Max texts to batch for encoding (default: 64)
#   ENCODE_BATCH_CHANNEL_SIZE   Buffer size for encode batch queue (default: 256)
#
# Override with docker-compose.override.yml or run directly:
#   docker run -p 8080:8080 --gpus all -v ~/.local/share/next-plaid:/data/indices \
#     next-plaid-api:cuda --model my-org/my-model --cuda --batch-size 128
# =============================================================================

services:
  next-plaid-api:
    build:
      context: .
      dockerfile: next-plaid-api/Dockerfile
      target: runtime-cuda
    volumes:
      # Persistent vector database storage (default: ~/.local/share/next-plaid)
      - ${NEXT_PLAID_DATA:-~/.local/share/next-plaid}:/data/indices
      # Persistent model cache (auto-downloaded from HuggingFace)
      - ${NEXT_PLAID_MODELS:-~/.cache/huggingface/next-plaid}:/models
    environment:
      - RUST_LOG=info
      - NVIDIA_VISIBLE_DEVICES=all
      # Rate limiting configuration
      - RATE_LIMIT_PER_SECOND=${RATE_LIMIT_PER_SECOND:-50}
      - RATE_LIMIT_BURST_SIZE=${RATE_LIMIT_BURST_SIZE:-100}
      - CONCURRENCY_LIMIT=${CONCURRENCY_LIMIT:-100}
      # Document processing configuration
      - MAX_QUEUED_TASKS_PER_INDEX=${MAX_QUEUED_TASKS_PER_INDEX:-10}
      - MAX_BATCH_DOCUMENTS=${MAX_BATCH_DOCUMENTS:-300}
      - BATCH_CHANNEL_SIZE=${BATCH_CHANNEL_SIZE:-100}
      # Encode batching configuration
      - MAX_BATCH_TEXTS=${MAX_BATCH_TEXTS:-64}
      - ENCODE_BATCH_CHANNEL_SIZE=${ENCODE_BATCH_CHANNEL_SIZE:-256}
    # CUDA defaults: FP32 model (GPU is fast), large batches, single session
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --index-dir
      - /data/indices
      - --model
      - ${MODEL:-lightonai/GTE-ModernColBERT-v1}
      - --cuda
      - --batch-size
      - "64"
      - --query-length
      - "48"
      - --document-length
      - "300"
    healthcheck:
      start_period: 120s  # Longer start period for model download + CUDA initialization
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
