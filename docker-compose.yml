# =============================================================================
# Next-Plaid API Docker Compose
# =============================================================================
# Default configuration with model support (CPU encoding).
# Use docker-compose.cuda.yml overlay for GPU encoding.
#
# Vector Database Storage:
#   Indices are persisted at ${NEXT_PLAID_DATA:-~/.local/share/next-plaid}
#   Each index is stored as a subdirectory: <data-dir>/<index-name>/
#   On container restart, existing indices are automatically loaded.
#
# Model Cache:
#   Downloaded HuggingFace models are cached at ${NEXT_PLAID_MODELS:-~/.cache/huggingface/next-plaid}
#   Models are only downloaded once and reused on subsequent container starts.
#
# Model Configuration (via command arguments):
#   --model <id>        HuggingFace model ID or local path
#   --int8              Use INT8 quantized model (~2x faster on CPU)
#   --parallel <N>      Number of parallel ONNX sessions
#   --batch-size <N>    Batch size per session
#   --threads <N>       Threads per session
#   --cuda              Use CUDA (for GPU builds)
#
# CPU Defaults (optimized for throughput):
#   --model lightonai/GTE-ModernColBERT-v1-onnx --int8 --parallel 8 --batch-size 4
#
# Examples:
#   # Default configuration
#   docker compose up -d
#
#   # Custom model with different parallel config (override command in docker-compose.override.yml)
#   # Or run directly:
#   docker run -p 8080:8080 -v ~/.local/share/next-plaid:/data/indices -v ~/.cache/huggingface/next-plaid:/models \
#     next-plaid-api --model my-org/my-model --parallel 16 --batch-size 2
#
# To customize storage locations, create a .env file with:
#   NEXT_PLAID_DATA=/path/to/indices
#   NEXT_PLAID_MODELS=/path/to/models
# =============================================================================

services:
  next-plaid-api:
    build:
      context: .
      dockerfile: next-plaid-api/Dockerfile
      target: runtime-cpu
    ports:
      - "8080:8080"
    volumes:
      # Persistent vector database storage
      # Default: ~/.local/share/next-plaid (XDG standard for user data)
      # Override with NEXT_PLAID_DATA environment variable
      - ${NEXT_PLAID_DATA:-~/.local/share/next-plaid}:/data/indices
      # Persistent model cache (auto-downloaded from HuggingFace)
      # Default: ~/.cache/huggingface (standard HF cache location)
      # Override with NEXT_PLAID_MODELS environment variable
      - ${NEXT_PLAID_MODELS:-~/.cache/huggingface/next-plaid}:/models
    environment:
      - RUST_LOG=info
    # CPU defaults: INT8 for ~2x speedup, 8 parallel sessions, batch size 4
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --index-dir
      - /data/indices
      - --model
      - lightonai/GTE-ModernColBERT-v1-onnx
      - --int8
      - --parallel
      - "8"
      - --batch-size
      - "4"
    healthcheck:
      test: ["CMD", "curl", "-f", "--max-time", "5", "http://localhost:8080/health"]
      interval: 15s
      timeout: 5s
      retries: 2
      start_period: 120s  # Longer start period for model download + loading
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G

